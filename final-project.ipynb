{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 5 - Vehicle Detection and Tracking\n",
    "\n",
    "##### By Matthew Zimmer - Future Self-Driving Car Engineer\n",
    "\n",
    "[GitHub](https://github.com/matthewzimmer) | [LinkedIn](https://www.linkedin.com/in/matthewazimmer)\n",
    "\n",
    "---\n",
    "\n",
    "**Vehicle Detection Project**\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images and train a classifier Linear SVM classifier\n",
    "* Optionally, you can also apply a color transform and append binned color features, as well as histograms of color, to your HOG feature vector. \n",
    "* Note: for those first two steps don't forget to normalize your features and randomize a selection for training and testing.\n",
    "* Implement a sliding-window technique and use your trained classifier to search for vehicles in images.\n",
    "* Run your pipeline on a video stream (start with the test_video.mp4 and later implement on full project_video.mp4) and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles.\n",
    "* Estimate a bounding box for vehicles detected.\n",
    "\n",
    "[//]: # (Image References)\n",
    "[image1]: ./examples/car_not_car.png\n",
    "[image2]: ./examples/HOG_example.jpg\n",
    "[image3]: ./examples/sliding_windows.jpg\n",
    "[image4]: ./examples/sliding_window.jpg\n",
    "[image5]: ./examples/bboxes_and_heat.png\n",
    "[image6]: ./examples/labels_map.png\n",
    "[image7]: ./examples/output_bboxes.png\n",
    "[video1]: ./project_video.mp4\n",
    "\n",
    "## [Rubric](https://review.udacity.com/#!/rubrics/513/view) Points\n",
    "\n",
    "### Here I will consider the rubric points individually and describe how I addressed each point in my implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Writeup / README\n",
    "\n",
    "#### 1. Provide a Writeup / README that includes all the rubric points and how you addressed each one.  You can submit your writeup as markdown or pdf.  [Here](https://github.com/udacity/CarND-Vehicle-Detection/blob/master/writeup_template.md) is a template writeup for this project you can use as a guide and a starting point.  \n",
    "\n",
    "You're reading it!\n",
    "\n",
    "> **CRITICAL NOTE** I treated this notebook as a comprehensive tutorial walking you from start to finish, top down, starting from calibrating the camera all the way to the very last step of drawing the detected lane region into the road. The very last cell of this notebook offers you the ability to control the hyper parameters used by my pipeline and either test them on a single image (the default setting) or test the sample the entire video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('qt5agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.display import Image, display\n",
    "\n",
    "from IPython.display import YouTubeVideo\n",
    "def render_youtube_video(video_id, width=880, height=495):\n",
    "    return YouTubeVideo(video_id, width=width, height=height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Pipeline Operations\n",
    "\n",
    "> **NOTE** Unless you absolutely want/need to check my code, you can totally skip over the next cell as it contains all of my classes used below.\n",
    "\n",
    "Pipeline operations are the principle driving force for this project. Each implementation of `PipelineOp` is a modular, reusable algorithm which, in its most basic form, performs a single operation on an image.\n",
    "\n",
    "`PipelineOp` has a simple interface with only 2 steps to satisfy the contract:\n",
    "\n",
    "1. Declare a constructor with inputs necessary to perform the operation in `#perform`.\n",
    "\n",
    "2. Implement `#perform`\n",
    "\n",
    "    * Your implementeation must return `self`. This provides support to perform the op and immediately assign the call to `#output` to local variables.\n",
    "\n",
    "    * Declared your op's output by calling `#_apply_output` once you've performed your operation.\n",
    "    \n",
    "The architecture does provide flexibility for you to implement more complicated algorithms that have many moving parts while still adhering to the contract by producing a single arbitrary output object (e.g., Dictionary, Image, Array). I have a health mixture of both simple and complex PipelineOp implementations in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PipelineOp:\n",
    "    def __init__(self):\n",
    "        self.__output = None\n",
    "\n",
    "    def perform(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def output(self):\n",
    "        return self.__output\n",
    "\n",
    "    def _apply_output(self, value):\n",
    "        self.__output = value\n",
    "        return self\n",
    "\n",
    "\n",
    "class CameraCalibrationOp(PipelineOp):\n",
    "    MODE_INITIALIZED = 0x0\n",
    "    MODE_CALIBRATING = 0x1\n",
    "    MODE_CALIBRATED = 0x2\n",
    "\n",
    "    # Bitmask of completed camera calibration stages\n",
    "    COMPLETED_CALIBRATION_STAGES = 0x0\n",
    "\n",
    "    STAGE_OBTAINED_CALIBRATION_IMAGES = 0x1 << 0\n",
    "    STAGE_COMPUTED_OBJ_AND_IMG_POINTS = 0x1 << 1\n",
    "    STAGE_CALCULATED_CAMERA_MTX_AND_DIST_COEFFICIENTS = 0x1 << 2\n",
    "    STAGE_UNDISTORED_CALIBRATION_IMAGES = 0x1 << 3\n",
    "    STAGE_SAVED_MTX_AND_DIST_CALIBRATIONS = 0x1 << 4\n",
    "\n",
    "    def __init__(self, calibration_images, x_inside_corners=9, y_inside_corners=6,\n",
    "                 calibration_results_pickle_file=\"camera_cal/camera_mtx_and_dist_pickle.p\"):\n",
    "        PipelineOp.__init__(self)\n",
    "\n",
    "        self.__mode = self.MODE_INITIALIZED\n",
    "\n",
    "        # Images taken by a camera for which this class calibrates by\n",
    "        # calculating the object and image points used to undistort\n",
    "        # any image take by the same camera.\n",
    "        self.__calibration_images = calibration_images\n",
    "\n",
    "        self.__x_inside_corners = x_inside_corners\n",
    "        self.__y_inside_corners = y_inside_corners\n",
    "\n",
    "        # Arrays to store object points and image points from all the images.\n",
    "        self.__objpoints = []  # 3d points in real world space\n",
    "        self.__imgpoints = []  # 2d points in image plane\n",
    "\n",
    "        # Computed using cv2.calibrateCamera() in __compute_camera_matrix_and_distortion_coefficients\n",
    "        self.__camera_matrix = None\n",
    "        self.__distortion_coefficients = None\n",
    "\n",
    "        # The location of the pickle file where our camera calibration matrix and\n",
    "        # distortion coefficients are persisted to\n",
    "        self.__calibration_results_pickle_file = calibration_results_pickle_file\n",
    "\n",
    "        self.__apply_stage(self.STAGE_OBTAINED_CALIBRATION_IMAGES)\n",
    "\n",
    "    def perform(self):\n",
    "        self.__mode = self.MODE_CALIBRATING\n",
    "        self.__compute_obj_and_img_points()\n",
    "        calibrations = self.__load_calibrations()\n",
    "        if calibrations is False:\n",
    "            self.__compute_camera_matrix_and_distortion_coefficients(self.__calibration_images[0])\n",
    "            self.__save_calibration_mtx_and_dist()\n",
    "        # self.__undistort_chessboard_images()\n",
    "        self.__mode = self.MODE_CALIBRATED\n",
    "        return self._apply_output({\n",
    "            'matrix': self.__camera_matrix,\n",
    "            'dist_coefficients': self.__distortion_coefficients,\n",
    "            'objpoints': self.__objpoints,\n",
    "            'imgpoints': self.__imgpoints\n",
    "        })\n",
    "\n",
    "    def undistort(self, img):\n",
    "        \"\"\"\n",
    "        A function that takes an image and performs the camera calibration,\n",
    "        image distortion correction and returns the undistorted image\n",
    "        \"\"\"\n",
    "        img = np.copy(img)\n",
    "        ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(self.__objpoints, self.__imgpoints, img.shape[0:2][::-1],\n",
    "                                                           None, None)\n",
    "        return cv2.undistort(img, mtx, dist, None, mtx)\n",
    "\n",
    "    # PRIVATE\n",
    "\n",
    "    def __detect_corners(self, img, nx, ny):\n",
    "        \"\"\"\n",
    "        This function converts an RGB chessboard image to grayscale and finds the\n",
    "        chessboard corners using cv2.findChessboardCorners.\n",
    "        \"\"\"\n",
    "        img = np.copy(img)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        return cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "\n",
    "    def __compute_obj_and_img_points(self):\n",
    "        \"\"\"\n",
    "        A function which iterates over all self.calibration_images and detects all\n",
    "        chessboard corners for each.\n",
    "\n",
    "        For each image corners are detected, a copy of that image with the corners\n",
    "        drawn on are saved to camera_cal/corners_found\n",
    "        \"\"\"\n",
    "        if not self.__is_stage_complete(self.STAGE_COMPUTED_OBJ_AND_IMG_POINTS):\n",
    "            nx, ny = self.__x_inside_corners, self.__y_inside_corners\n",
    "\n",
    "            # prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "            objp = np.zeros((nx * ny, 3), np.float32)\n",
    "            objp[:, :2] = np.mgrid[0:nx, 0:ny].T.reshape(-1, 2)\n",
    "\n",
    "            # Step through the list and search for chessboard corners\n",
    "            for fname in self.__calibration_images:\n",
    "                img = mpimg.imread(fname)\n",
    "                ret, corners = self.__detect_corners(img, nx, ny)\n",
    "\n",
    "                # If found, add object points, image points\n",
    "                if ret == True:\n",
    "                    self.__objpoints.append(objp)\n",
    "                    self.__imgpoints.append(corners)\n",
    "\n",
    "                    # print(\"{} corners detected\".format(os.path.basename(fname)))\n",
    "                    calibrated_name = 'camera_cal/corners_found/{}'.format(str(os.path.basename(fname)))\n",
    "                    cv2.drawChessboardCorners(img, (nx, ny), corners, ret)\n",
    "                    cv2.imwrite(calibrated_name, img)\n",
    "\n",
    "            self.__apply_stage(self.STAGE_COMPUTED_OBJ_AND_IMG_POINTS)\n",
    "\n",
    "    def __undistort_chessboard_images(self):\n",
    "        if self.__is_stage_complete(self.STAGE_COMPUTED_OBJ_AND_IMG_POINTS) and not self.__is_stage_complete(\n",
    "                self.STAGE_UNDISTORED_CALIBRATION_IMAGES):\n",
    "            # Step through the list and search for chessboard corners\n",
    "            for fname in self.__calibration_images:\n",
    "                img = mpimg.imread(fname)\n",
    "                undistorted = self.undistort(img)\n",
    "\n",
    "                # print(\"{} undistorted\".format(os.path.basename(fname)))\n",
    "                undist_file = 'camera_cal/undistorted/{}'.format(os.path.basename(fname))\n",
    "                cv2.imwrite(undist_file, undistorted)\n",
    "\n",
    "            self.__apply_stage(self.STAGE_UNDISTORED_CALIBRATION_IMAGES)\n",
    "\n",
    "    def __load_calibrations(self):\n",
    "        if os.path.isfile(self.__calibration_results_pickle_file):\n",
    "            with open(self.__calibration_results_pickle_file, 'rb') as f:\n",
    "                pickle_data = pickle.load(f)\n",
    "                self.__camera_matrix = pickle_data['mtx']\n",
    "                self.__distortion_coefficients = pickle_data['dist']\n",
    "                if self.__camera_matrix is not None:\n",
    "                    self.__apply_stage(self.STAGE_CALCULATED_CAMERA_MTX_AND_DIST_COEFFICIENTS)\n",
    "                    self.__apply_stage(self.STAGE_SAVED_MTX_AND_DIST_CALIBRATIONS)\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def __compute_camera_matrix_and_distortion_coefficients(self, distorted_image_path):\n",
    "        if self.__is_stage_complete(self.STAGE_COMPUTED_OBJ_AND_IMG_POINTS) and not self.__is_stage_complete(\n",
    "                self.STAGE_CALCULATED_CAMERA_MTX_AND_DIST_COEFFICIENTS):\n",
    "            fname = distorted_image_path\n",
    "            img = mpimg.imread(fname)\n",
    "            ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(self.__objpoints, self.__imgpoints, img.shape[0:2][::-1],\n",
    "                                                               None, None)\n",
    "\n",
    "            self.__camera_matrix = mtx\n",
    "            self.__distortion_coefficients = dist\n",
    "\n",
    "            self.__apply_stage(self.STAGE_CALCULATED_CAMERA_MTX_AND_DIST_COEFFICIENTS)\n",
    "\n",
    "    def __save_calibration_mtx_and_dist(self):\n",
    "        \"\"\"\n",
    "        Saves a pickled representation of the camera calibration matrix and\n",
    "        distortion coefficient results for the provided image for later use\n",
    "        \"\"\"\n",
    "        if self.__is_stage_complete(self.STAGE_COMPUTED_OBJ_AND_IMG_POINTS) and not self.__is_stage_complete(\n",
    "                self.STAGE_SAVED_MTX_AND_DIST_CALIBRATIONS):\n",
    "            dist_pickle = {}\n",
    "            dist_pickle[\"mtx\"] = self.__camera_matrix\n",
    "            dist_pickle[\"dist\"] = self.__distortion_coefficients\n",
    "            pickle.dump(dist_pickle, open(self.__calibration_results_pickle_file, \"wb\"))\n",
    "\n",
    "            # print('camera matrix and distortion coefficients pickled to \"{}\" for later use'.format(self.__calibration_results_pickle_file))\n",
    "\n",
    "            self.__apply_stage(self.STAGE_SAVED_MTX_AND_DIST_CALIBRATIONS)\n",
    "\n",
    "    def __is_stage_complete(self, flag):\n",
    "        return self.COMPLETED_CALIBRATION_STAGES & flag == flag\n",
    "\n",
    "    def __apply_stage(self, flag):\n",
    "        \"\"\"Marks a stage as complete\"\"\"\n",
    "        self.COMPLETED_CALIBRATION_STAGES = self.COMPLETED_CALIBRATION_STAGES | flag\n",
    "\n",
    "    def __str__(self):\n",
    "        s = []\n",
    "\n",
    "        s.append('')\n",
    "        s.append('')\n",
    "        s.append('-------------------------------------------------------------')\n",
    "        s.append('')\n",
    "        s.append('[ CALIBRATION MODES ]')\n",
    "        s.append('')\n",
    "        s.append('   Initialized? {}'.format('YES' if self.__mode == self.MODE_INITIALIZED else 'NO'))\n",
    "        s.append('   Calibrating? {}'.format('YES' if self.__mode == self.MODE_CALIBRATING else 'NO'))\n",
    "        s.append('   Calibration complete? {}'.format('YES' if self.__mode == self.MODE_CALIBRATED else 'NO'))\n",
    "        s.append('')\n",
    "        s.append('')\n",
    "\n",
    "        s.append('[ CALIBRATION STAGES - {} ]'.format(self.COMPLETED_CALIBRATION_STAGES))\n",
    "        s.append('')\n",
    "        s.append('   Obtained calibration images? {}'.format(\n",
    "            'YES' if self.__is_stage_complete(self.STAGE_OBTAINED_CALIBRATION_IMAGES) else 'NO'))\n",
    "        s.append('   Computed object/image points? {}'.format(\n",
    "            'YES' if self.__is_stage_complete(self.STAGE_COMPUTED_OBJ_AND_IMG_POINTS) else 'NO'))\n",
    "        s.append('   Calculated camera matrix and distortion coefficients? {}'.format(\n",
    "            'YES' if self.__is_stage_complete(self.STAGE_CALCULATED_CAMERA_MTX_AND_DIST_COEFFICIENTS) else 'NO'))\n",
    "        s.append('   Undistored calibration images? {}'.format(\n",
    "            'YES' if self.__is_stage_complete(self.STAGE_UNDISTORED_CALIBRATION_IMAGES) else 'NO'))\n",
    "        s.append('   Persisted camera matrix and distortion coefficients? {}'.format(\n",
    "            'YES' if self.__is_stage_complete(self.STAGE_SAVED_MTX_AND_DIST_CALIBRATIONS) else 'NO'))\n",
    "        s.append('')\n",
    "\n",
    "        s.append('[ PARAMS ]')\n",
    "        s.append('')\n",
    "        s.append('Number calibration images: {}'.format(len(self.__calibration_images)))\n",
    "        s.append('X inside corners = {}'.format(self.__x_inside_corners))\n",
    "        s.append('Y inside corners = {}'.format(self.__y_inside_corners))\n",
    "        s.append('')\n",
    "        # s.append('output = {}'.format(str(self.output())))\n",
    "\n",
    "        s.append('')\n",
    "        s.append('')\n",
    "\n",
    "        return '\\n'.join(s)\n",
    "\n",
    "\n",
    "class ConvertColorSpaceOp(PipelineOp):\n",
    "    def __init__(self, img, color_space, src_color_space='RGB', color_channel=-1):\n",
    "        \"\"\"\n",
    "        Converts an image to a different color space.\n",
    "        \n",
    "        Available color spaces: HSV, HLS, YUV, GRAY, YCrCb\n",
    "        \"\"\"\n",
    "        PipelineOp.__init__(self)\n",
    "        self.__img = np.copy(img)\n",
    "        self.__color_space = color_space.upper()\n",
    "        self.__src_color_space = src_color_space.upper()\n",
    "        self.__color_channel = color_channel\n",
    "    \n",
    "    def perform(self):\n",
    "        img = cv2.cvtColor(self.__img, eval('cv2.COLOR_{}2{}'.format(self.__src_color_space, self.__color_space))).astype(np.float)\n",
    "        if self.__color_channel > -1:\n",
    "            img = img[:,:,self.__color_channel]\n",
    "        return self._apply_output(img)\n",
    "\n",
    "\n",
    "class ColorThreshOp(PipelineOp):\n",
    "    def __init__(self, gray_img, color_thresh=(0, 255)):\n",
    "        PipelineOp.__init__(self)\n",
    "        self.__img = np.copy(gray_img)\n",
    "        self.__color_thresh = color_thresh\n",
    "    \n",
    "    def perform(self):\n",
    "        # ret, thresholded_img = cv2.threshold(img.astype('uint8'), self._color_thresh[0], self._color_thresh[1], cv2.THRESH_BINARY)\n",
    "        # self._thresholded_img = thresholded_img\n",
    "        # self._binary_img = binary_img\n",
    "        binary = np.zeros_like(self.__img)\n",
    "        binary[(self.__img > self.__color_thresh[0]) & (self.__img <= self.__color_thresh[1])] = 1\n",
    "        return self._apply_output(binary)\n",
    "\n",
    "\n",
    "class UndistortOp(PipelineOp):\n",
    "    def __init__(self, img, camera_calibration_op):\n",
    "        \"\"\"\n",
    "        Takes an image and cam and performs image distortion correction\n",
    "        \"\"\"\n",
    "        PipelineOp.__init__(self)\n",
    "        self.__img = np.copy(img)\n",
    "        self.__camera_calibration_op = camera_calibration_op\n",
    "    \n",
    "    def perform(self):\n",
    "        return self._apply_output(self.__camera_calibration_op.undistort(self.__img))\n",
    "\n",
    "        \n",
    "class SobelThreshOp(PipelineOp):\n",
    "    def __init__(self, gray, orient='x', sobel_kernel=3, thresh=(0, 255)):\n",
    "        PipelineOp.__init__(self)\n",
    "        self.__img = np.copy(gray)\n",
    "        self.__orient = orient\n",
    "        self.__sobel_kernel = sobel_kernel # Choose a larger odd number to smooth gradient measurements\n",
    "        self.__thresh = thresh\n",
    "        \n",
    "    def perform(self):\n",
    "        gray = self.__img\n",
    "        sobel = cv2.Sobel(gray, cv2.CV_64F, self.__orient=='x', self.__orient!='x', ksize=self.__sobel_kernel)\n",
    "        abs_sobel = np.absolute(sobel)\n",
    "        scaled_sobel = (255*abs_sobel/np.max(abs_sobel)).astype(np.uint8) \n",
    "        binary = np.zeros_like(scaled_sobel)\n",
    "        binary[(scaled_sobel >= self.__thresh[0]) & (scaled_sobel <= self.__thresh[1])] = 1\n",
    "        return self._apply_output(binary)\n",
    "\n",
    "        \n",
    "class MagnitudeGradientThreshOp(PipelineOp):\n",
    "    def __init__(self, gray_img, sobel_kernel=3, thresh=(0, 255)):\n",
    "        PipelineOp.__init__(self)\n",
    "        self.__img = np.copy(gray_img)\n",
    "        self.__sobel_kernel = sobel_kernel # Choose a larger odd number to smooth gradient measurements\n",
    "        self.__thresh = thresh\n",
    "        \n",
    "    def perform(self):\n",
    "        gray = self.__img\n",
    "        sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=self.__sobel_kernel)\n",
    "        sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=self.__sobel_kernel)\n",
    "        gradmag = np.sqrt(sobelx**2 + sobely**2)\n",
    "        gradmag = (255*gradmag/np.max(gradmag)).astype(np.uint8) \n",
    "        binary = np.zeros_like(gradmag)\n",
    "        binary[(gradmag >= self.__thresh[0]) & (gradmag <= self.__thresh[1])] = 1\n",
    "        return self._apply_output(binary)\n",
    "\n",
    "        \n",
    "class DirectionGradientThreshOp(PipelineOp):\n",
    "    \"\"\"\n",
    "    Calculates the gradient direction of detected lines \n",
    "    \"\"\"\n",
    "    def __init__(self, gray_img, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "        PipelineOp.__init__(self)\n",
    "        self.__img = np.copy(gray_img)\n",
    "        self.__sobel_kernel = sobel_kernel # Choose a larger odd number to smooth gradient measurements\n",
    "        self.__thresh = thresh\n",
    "        \n",
    "    def perform(self):\n",
    "        sobelx = cv2.Sobel(self.__img, cv2.CV_64F, 1, 0, ksize=self.__sobel_kernel)\n",
    "        sobely = cv2.Sobel(self.__img, cv2.CV_64F, 0, 1, ksize=self.__sobel_kernel)\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            abs_grad_dir = np.absolute(np.arctan(sobely/sobelx))\n",
    "            binary = np.zeros_like(abs_grad_dir)\n",
    "            binary[(abs_grad_dir > self.__thresh[0]) & (abs_grad_dir < self.__thresh[1])] = 1\n",
    "        return self._apply_output(binary)\n",
    "\n",
    "        \n",
    "class WarperOp(PipelineOp):\n",
    "    \n",
    "    def __init__(self, gray_img, src_pts, dst_pts):\n",
    "        PipelineOp.__init__(self)\n",
    "        self.__img = np.copy(gray_img)\n",
    "        self.__src_pts = src_pts\n",
    "        self.__dst_pts = dst_pts\n",
    "    \n",
    "    def perform(self):\n",
    "        # Compute the perspective transform, M, given source and destination points:\n",
    "        M = cv2.getPerspectiveTransform(self.__src_pts, self.__dst_pts)\n",
    "        \n",
    "        # Compute the inverse perspective transform:\n",
    "        Minv = cv2.getPerspectiveTransform(self.__dst_pts, self.__src_pts)\n",
    "        \n",
    "        # Warp an image using the perspective transform, M:\n",
    "        warped = cv2.warpPerspective(self.__img, M, self.__img.shape[0:2][::-1], flags=cv2.INTER_LINEAR)\n",
    "        \n",
    "        return self._apply_output({\n",
    "            'warped': warped,\n",
    "            'M': M, \n",
    "            'Minv': Minv,\n",
    "            'src_pts': self.__src_pts,\n",
    "            'dst_pts': self.__dst_pts\n",
    "        })\n",
    "    \n",
    "    def __str__(self):\n",
    "        s = []\n",
    "\n",
    "        s.append(' source image shape: ')\n",
    "        s.append('')\n",
    "        s.append('   '+str(self.__img.shape))\n",
    "        s.append('')\n",
    "\n",
    "        s.append(' source points: ')\n",
    "        s.append('')\n",
    "        s.append('   top.L: '+str(self.__src_pts[0]))\n",
    "        s.append('   bot.L: '+str(self.__src_pts[1]))\n",
    "        s.append('   bot.R: '+str(self.__src_pts[2]))\n",
    "        s.append('   top.R: '+str(self.__src_pts[3]))\n",
    "        s.append('')\n",
    "\n",
    "        s.append(' desination points: ')\n",
    "        s.append('')\n",
    "        s.append('   top.L: '+str(self.__dst_pts[0]))\n",
    "        s.append('   bot.L: '+str(self.__dst_pts[1]))\n",
    "        s.append('   bot.R: '+str(self.__dst_pts[2]))\n",
    "        s.append('   top.R: '+str(self.__dst_pts[3]))\n",
    "        s.append('')\n",
    "        s.append('')\n",
    "        \n",
    "        return '\\n'.join(s)\n",
    "    \n",
    "class PlotImageOp(PipelineOp):\n",
    "    def __init__(self, img, title='', cmap='gray', interpolation='none', aspect='auto'):\n",
    "        PipelineOp.__init__(self)\n",
    "        self.__img = np.copy(img)\n",
    "        self.__title = title\n",
    "        self.__cmap = cmap\n",
    "        self.__interpolation = interpolation\n",
    "        self.__aspect = aspect\n",
    "\n",
    "    def perform(self):\n",
    "        fig1 = plt.figure(figsize=(16,9))\n",
    "        ax = fig1.add_subplot(111)\n",
    "        ax.imshow(self.__img, cmap=self.__cmap, interpolation=self.__interpolation, aspect=self.__aspect)\n",
    "        plt.tight_layout()\n",
    "        ax.set_title(self.__title)\n",
    "        plt.show()\n",
    "        return self._apply_output(ax)\n",
    "\n",
    "class DrawPolyLinesOp(PipelineOp):\n",
    "    def __init__(self, img, pts, color=(0, 140, 255), thickness=5):\n",
    "        PipelineOp.__init__(self)\n",
    "        self.__img = np.copy(img)\n",
    "        self.__pts = pts\n",
    "        self.__color = color\n",
    "        self.__thickness = thickness\n",
    "    \n",
    "    def perform(self):\n",
    "        return self._apply_output(cv2.polylines(self.__img, [np.array([self.__pts], np.int32)], True, self.__color, thickness=self.__thickness))\n",
    "    \n",
    "class PolyfitLineOp(PipelineOp):\n",
    "    def __init__(self, binary_warped):\n",
    "        PipelineOp.__init__(self)\n",
    "        self.__binary_warped = binary_warped\n",
    "    \n",
    "    def perform(self):\n",
    "        binary_warped = self.__binary_warped\n",
    "        \n",
    "        # Takes a histogram of the bottom half of a warped binary image\n",
    "        histogram = np.sum(binary_warped[binary_warped.shape[0]/2:,:], axis=0)\n",
    "\n",
    "        # Create an output image to draw on and  visualize the result\n",
    "        out_img = np.dstack((binary_warped, binary_warped, binary_warped))*255\n",
    "\n",
    "        # Find the peak of the left and right halves of the histogram\n",
    "        # These will be the starting point for the left and right lines\n",
    "        midpoint = np.int(histogram.shape[0]/2)\n",
    "        leftx_base = np.argmax(histogram[:midpoint])\n",
    "        rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "\n",
    "        # Choose the number of sliding windows\n",
    "        nwindows = 9\n",
    "        # Set height of windows\n",
    "        window_height = np.int(binary_warped.shape[0]/nwindows)\n",
    "        # Identify the x and y positions of all nonzero pixels in the image\n",
    "        nonzero = binary_warped.nonzero()\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Current positions to be updated for each window\n",
    "        leftx_current = leftx_base\n",
    "        rightx_current = rightx_base\n",
    "        # Set the width of the windows +/- margin\n",
    "        margin = 100\n",
    "        # Set minimum number of pixels found to recenter window\n",
    "        minpix = 50\n",
    "        # Create empty lists to receive left and right lane pixel indices\n",
    "        left_lane_inds = []\n",
    "        right_lane_inds = []\n",
    "\n",
    "        # Step through the windows one by one\n",
    "        for window in range(nwindows):\n",
    "            # Identify window boundaries in x and y (and right and left)\n",
    "            win_y_low = binary_warped.shape[0] - (window+1)*window_height\n",
    "            win_y_high = binary_warped.shape[0] - window*window_height\n",
    "            win_xleft_low = leftx_current - margin\n",
    "            win_xleft_high = leftx_current + margin\n",
    "            win_xright_low = rightx_current - margin\n",
    "            win_xright_high = rightx_current + margin\n",
    "            # Draw the windows on the visualization image\n",
    "            cv2.rectangle(out_img,(win_xleft_low,win_y_low),(win_xleft_high,win_y_high),(0,255,0), 2) \n",
    "            cv2.rectangle(out_img,(win_xright_low,win_y_low),(win_xright_high,win_y_high),(0,255,0), 2) \n",
    "            # Identify the nonzero pixels in x and y within the window\n",
    "            good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xleft_low) & (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "            good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xright_low) & (nonzerox < win_xright_high)).nonzero()[0]\n",
    "            # Append these indices to the lists\n",
    "            left_lane_inds.append(good_left_inds)\n",
    "            right_lane_inds.append(good_right_inds)\n",
    "            # If you found > minpix pixels, recenter next window on their mean position\n",
    "            if len(good_left_inds) > minpix:\n",
    "                leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "            if len(good_right_inds) > minpix:        \n",
    "                rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "\n",
    "        # Concatenate the arrays of indices\n",
    "        left_lane_inds = np.concatenate(left_lane_inds)\n",
    "        right_lane_inds = np.concatenate(right_lane_inds)\n",
    "\n",
    "        # Extract left and right line pixel positions\n",
    "        leftx = nonzerox[left_lane_inds]\n",
    "        lefty = nonzeroy[left_lane_inds] \n",
    "        rightx = nonzerox[right_lane_inds]\n",
    "        righty = nonzeroy[right_lane_inds] \n",
    "\n",
    "        # Fit a second order polynomial to each\n",
    "        left_fit = np.polyfit(lefty, leftx, 2)\n",
    "        right_fit = np.polyfit(righty, rightx, 2)\n",
    "        \n",
    "        return self._apply_output({\n",
    "            'left_fit': left_fit,\n",
    "            'right_fit': right_fit,\n",
    "            'out_img': out_img,\n",
    "            'left_lane_inds': left_lane_inds,\n",
    "            'right_lane_inds': right_lane_inds,\n",
    "            'leftx': leftx,\n",
    "            'lefty': lefty,\n",
    "            'rightx': rightx,\n",
    "            'righty': righty,\n",
    "            'nonzerox': nonzerox,\n",
    "            'nonzeroy': nonzeroy\n",
    "        })\n",
    "    \n",
    "    \n",
    "# A class to receive the characteristics of each line detection\n",
    "#\n",
    "#   **** >>>> UNIMPLEMENTED...NOT USED BY MY PIPELINE FOR ANYTHING WORTH NOTING\n",
    "#\n",
    "class LineOp(PipelineOp):\n",
    "    # This constant ultimately contributes to deriving a given\n",
    "    # period when computing SMA and EMA for line noise smoothing\n",
    "    FPS = 30\n",
    "    \n",
    "    def __init__(self, ema_period_alpha=0.65):\n",
    "        PipelineOp.__init__(self)\n",
    "        self.__ema_fps_period = ema_period_alpha * self.FPS\n",
    "        self.__all_measurements = np.array([])\n",
    "        \n",
    "        # was the line detected in the last iteration?\n",
    "        self.detected = False  \n",
    "        # x values of the last n fits of the line\n",
    "        self.recent_xfitted = [] \n",
    "        #average x values of the fitted line over the last n iterations\n",
    "        self.bestx = None     \n",
    "        #polynomial coefficients averaged over the last n iterations\n",
    "        self.best_fit = None  \n",
    "        #polynomial coefficients for the most recent fit\n",
    "        self.current_fit = [np.array([False])]  \n",
    "        #radius of curvature of the line in some units\n",
    "        self.radius_of_curvature = None \n",
    "        #distance in meters of vehicle center from the line\n",
    "        self.line_base_pos = None \n",
    "        #difference in fit coefficients between last and new fits\n",
    "        self.diffs = np.array([0,0,0], dtype='float') \n",
    "        #x values for detected line pixels\n",
    "        self.allx = np.array([])\n",
    "        #y values for detected line pixels\n",
    "        self.ally = np.array([])\n",
    "        \n",
    "    def perform(self):\n",
    "        # self.__compute_ema(x, self.allx)\n",
    "        return self\n",
    "    \n",
    "    def add_measurement(self, x, y, radius_of_curvature, line_base_pos):\n",
    "        self.__all_measurements = np.append(self.__all_measurements, (x, y, radius_of_curvature, line_base_pos))\n",
    "        self.allx = np.append(self.allx, x)\n",
    "        self.ally = np.append(self.ally, y)\n",
    "        self.radius_of_curvature = radius_of_curvature\n",
    "        self.line_base_pos = line_base_pos\n",
    "        return self\n",
    "    \n",
    "    def __compute_ema(self, measurement, all_measurements, curr_ema):\n",
    "        sma = sum(all_measurements) / (len(all_measurements))\n",
    "\n",
    "        if len(all_measurements) < self.__ema_fps_period:\n",
    "            # let's just use SMA until\n",
    "            # our EMA buffer is filled\n",
    "            return sma\n",
    "\n",
    "        multiplier = 2 / float(len(all_measurements) + 1)\n",
    "        ema = (measurement - curr_ema) * multiplier + curr_ema\n",
    "\n",
    "        # print(\"sma: %s, multiplier: %s\" % (sma, multiplier))\n",
    "        return ema\n",
    "\n",
    "\n",
    "class LaneAssistOp(PipelineOp):\n",
    "    def __init__(self,\n",
    "                 calibration_op,\n",
    "                 margin=100, \n",
    "                 kernel_size=3, \n",
    "                 sobelx_thresh=(20, 100), \n",
    "                 sobely_thresh=(20, 100),\n",
    "                 mag_grad_thresh=(20, 250), \n",
    "                 dir_grad_thresh=(0., np.pi / 2),\n",
    "                 color_space='HSV',\n",
    "                 color_channel=2\n",
    "                ):\n",
    "        PipelineOp.__init__(self)\n",
    "        self.__margin = margin\n",
    "        self.__calibration_op = calibration_op\n",
    "        self.__kernel_size = kernel_size\n",
    "        self.__sobelx_thresh = sobelx_thresh\n",
    "        self.__sobely_thresh = sobely_thresh\n",
    "        self.__mag_grad_thresh = mag_grad_thresh\n",
    "        self.__dir_grad_thresh = dir_grad_thresh\n",
    "        self.__left_line = LineOp()\n",
    "        self.__right_line = LineOp()\n",
    "        self.__save_counter = 0\n",
    "        self.__polyfit_op = None\n",
    "        self.__color_space = color_space\n",
    "        self.__color_channel = color_channel\n",
    "    \n",
    "    def __save_image(self, img, name):\n",
    "        self.__save_counter += 1\n",
    "        # cv2.imwrite('processed_images/{}_{}_{}.jpg'.format(self.__name, self.__save_counter, name), img)\n",
    "    \n",
    "    def process_image(self, img, name):\n",
    "        self.__save_counter = 0\n",
    "        self.__img = np.copy(img)\n",
    "        self.__name = name\n",
    "        return self.perform()\n",
    "    \n",
    "    def perform(self):\n",
    "        img = self.__img\n",
    "        kernel_size = self.__kernel_size\n",
    "        sobelx_thresh = self.__sobelx_thresh\n",
    "        sobely_thresh = self.__sobely_thresh\n",
    "        mag_grad_thresh = self.__mag_grad_thresh\n",
    "        dir_grad_thresh = self.__dir_grad_thresh\n",
    "\n",
    "        # undistort the raw image\n",
    "        undistorted = UndistortOp(img, self.__calibration_op).perform().output()\n",
    "        self.__save_image(cv2.cvtColor(undistorted, cv2.COLOR_RGB2BGR), 'undistorted')\n",
    "\n",
    "        # Convert undistored image to HSV and use the 'V' channel as our gray image.\n",
    "        color_cvt = ConvertColorSpaceOp(undistorted, color_space=self.__color_space, color_channel=self.__color_channel).perform().output()\n",
    "        self.__save_image(color_cvt, '{} channel {}'.format(self.__color_space, self.__color_channel))\n",
    "\n",
    "        # Compute sobel X binary image\n",
    "        gradx = SobelThreshOp(color_cvt, orient='x', sobel_kernel=kernel_size, thresh=sobelx_thresh).perform().output()\n",
    "        self.__save_image(gradx*255, 'gradx')\n",
    "\n",
    "        # Compute sobel Y binary image\n",
    "        grady = SobelThreshOp(color_cvt, orient='y', sobel_kernel=kernel_size, thresh=sobely_thresh).perform().output()\n",
    "        self.__save_image(grady*255, 'grady')\n",
    "\n",
    "        # Compute Magnitude Gradient binary image\n",
    "        mag_binary = MagnitudeGradientThreshOp(color_cvt, sobel_kernel=kernel_size, thresh=mag_grad_thresh).perform().output()\n",
    "        self.__save_image(mag_binary*255, 'mag_binary')\n",
    "\n",
    "        # Compute Direction Gradient binary image\n",
    "        dir_binary = DirectionGradientThreshOp(color_cvt, sobel_kernel=kernel_size, thresh=dir_grad_thresh).perform().output()\n",
    "        self.__save_image(dir_binary*255, 'dir_binary')\n",
    "\n",
    "        # Perform bitwise AND and OR to create a final binary image where we generate a binary image of\n",
    "        # all white pixels in (SobelX AND SobelY) and combine it via binary OR with a binary image of all white pixels\n",
    "        # in (Magnitude AND Direction) gradients.\n",
    "        combined = np.zeros_like(color_cvt)\n",
    "        combined[((gradx == 1) & (grady == 1)) | ((mag_binary == 1) & (dir_binary == 1))] = 1\n",
    "        self.__save_image(combined*255, 'final_binary_thresh')\n",
    "\n",
    "        # Now we're going to warp our combined threshholded binary image\n",
    "        img_size = (img.shape[1], img.shape[0])\n",
    "        \n",
    "        src_pts = np.float32(\n",
    "            [[(img_size[0] / 2) - 55, img_size[1] / 2 + 100],\n",
    "            [((img_size[0] / 6) - 55), img_size[1]],\n",
    "            [(img_size[0] * 5 / 6) + 60, img_size[1]],\n",
    "            [(img_size[0] / 2 + 65), img_size[1] / 2 + 100]])\n",
    "\n",
    "        dst_pts = np.float32(\n",
    "            [[(img_size[0] / 6), 0],\n",
    "             [(img_size[0] / 6), img_size[1]],\n",
    "             [(img_size[0] * 5 / 6), img_size[1]],\n",
    "             [(img_size[0] * 5 / 6), 0]])\n",
    "\n",
    "        warper_op = WarperOp(combined, src_pts, dst_pts).perform().output()\n",
    "        binary_warped = warper_op['warped']\n",
    "        self.__save_image(binary_warped*255, 'binary_warped')\n",
    "\n",
    "        if True or self.__polyfit_op is None:\n",
    "            self.__polyfit_op = PolyfitLineOp(binary_warped).perform().output()\n",
    "        \n",
    "        left_fit = self.__polyfit_op['left_fit']\n",
    "        right_fit = self.__polyfit_op['right_fit']\n",
    "        nonzeroy = self.__polyfit_op['nonzeroy']\n",
    "        nonzerox = self.__polyfit_op['nonzerox']\n",
    "        left_lane_inds = self.__polyfit_op['left_lane_inds']\n",
    "        right_lane_inds = self.__polyfit_op['right_lane_inds']\n",
    "\n",
    "        # Generate x and y values for plotting\n",
    "        fity = np.linspace(0, binary_warped.shape[0] - 1, binary_warped.shape[0])\n",
    "        fit_leftx = left_fit[0] * fity ** 2 + left_fit[1] * fity + left_fit[2]\n",
    "        fit_rightx = right_fit[0] * fity ** 2 + right_fit[1] * fity + right_fit[2]\n",
    "\n",
    "        out_img = self.__polyfit_op['out_img']\n",
    "        out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "        out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "\n",
    "        # Assume you now have a new warped binary image\n",
    "        # from the next frame of video (also called \"binary_warped\")\n",
    "        # It's now much easier to find line pixels!\n",
    "        nonzero = binary_warped.nonzero()\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "\n",
    "        margin = self.__margin\n",
    "        left_lane_inds = (\n",
    "        (nonzerox > (left_fit[0] * (nonzeroy ** 2) + left_fit[1] * nonzeroy + left_fit[2] - margin)) & (\n",
    "        nonzerox < (left_fit[0] * (nonzeroy ** 2) + left_fit[1] * nonzeroy + left_fit[2] + margin)))\n",
    "        \n",
    "        right_lane_inds = (\n",
    "        (nonzerox > (right_fit[0] * (nonzeroy ** 2) + right_fit[1] * nonzeroy + right_fit[2] - margin)) & (\n",
    "        nonzerox < (right_fit[0] * (nonzeroy ** 2) + right_fit[1] * nonzeroy + right_fit[2] + margin)))\n",
    "\n",
    "        # Again, extract left and right line pixel positions\n",
    "        leftx = nonzerox[left_lane_inds]\n",
    "        lefty = nonzeroy[left_lane_inds]\n",
    "        rightx = nonzerox[right_lane_inds]\n",
    "        righty = nonzeroy[right_lane_inds]\n",
    "        # Fit a second order polynomial to each\n",
    "        left_fit = np.polyfit(lefty, leftx, 2)\n",
    "        right_fit = np.polyfit(righty, rightx, 2)\n",
    "        # Generate x and y values for plotting\n",
    "        fity = np.linspace(0, binary_warped.shape[0] - 1, binary_warped.shape[0])\n",
    "        fit_leftx = left_fit[0] * fity ** 2 + left_fit[1] * fity + left_fit[2]\n",
    "        fit_rightx = right_fit[0] * fity ** 2 + right_fit[1] * fity + right_fit[2]\n",
    "\n",
    "        # Create an image to draw on and an image to show the selection window\n",
    "        out_img = np.dstack((binary_warped, binary_warped, binary_warped)) * 255\n",
    "        window_img = np.zeros_like(out_img)\n",
    "        # Color in left and right line pixels\n",
    "        out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "        out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "\n",
    "        # Generate a polygon to illustrate the search window area\n",
    "        # And recast the x and y points into usable format for cv2.fillPoly()\n",
    "        left_line_window1 = np.array([np.transpose(np.vstack([fit_leftx - margin, fity]))])\n",
    "        left_line_window2 = np.array([np.flipud(np.transpose(np.vstack([fit_leftx + margin, fity])))])\n",
    "        left_line_pts = np.hstack((left_line_window1, left_line_window2))\n",
    "        right_line_window1 = np.array([np.transpose(np.vstack([fit_rightx - margin, fity]))])\n",
    "        right_line_window2 = np.array([np.flipud(np.transpose(np.vstack([fit_rightx + margin, fity])))])\n",
    "        right_line_pts = np.hstack((right_line_window1, right_line_window2))\n",
    "\n",
    "        # Define y-value where we want radius of curvature\n",
    "        # I'll choose the maximum y-value, corresponding to the bottom of the image\n",
    "        y_eval = out_img.shape[0]\n",
    "\n",
    "        left_curverad = ((1 + (2 * left_fit[0] * y_eval + left_fit[1]) ** 2) ** 1.5) / np.absolute(2 * left_fit[0])\n",
    "        right_curverad = ((1 + (2 * right_fit[0] * y_eval + right_fit[1]) ** 2) ** 1.5) / np.absolute(2 * right_fit[0])\n",
    "\n",
    "        #print('left:', left_curverad, '| right:', right_curverad)\n",
    "\n",
    "        # Define conversions in x and y from pixels space to meters\n",
    "        ym_per_pix = 30 / out_img.shape[0]  # meters per pixel in y dimension\n",
    "        xm_per_pix = 3.7 / (out_img.shape[0] - 20)  # meteres per pixel in x dimension\n",
    "\n",
    "        # Find center of car relitive to center of lane\n",
    "        #determine center of the lane. \n",
    "        left_line_baseX = left_fit[2] # Lesson 12 - distance from the bottom(y=720) left(x=0) of the image to the left lane line in pixels.\n",
    "        left_line_baseMeters = left_line_baseX * xm_per_pix\n",
    "        right_line_baseX = right_fit[2]#same for the right lane line.  \n",
    "        right_line_baseMeters = right_line_baseX * xm_per_pix\n",
    "        lane_center_pixels = left_line_baseX+(0.5*(right_line_baseX-left_line_baseX))#in pixels\n",
    "        #convert to meters\n",
    "        lane_center_meters = lane_center_pixels*xm_per_pix # pixels*meter/pixel = meter\n",
    "        car_center_pixels = (out_img.shape[1]/2) + 43 # 683 pixels from the bottom left of the image to the center of the car in pixels\n",
    "        #Convert to meters.\n",
    "        #meters from left of image\n",
    "        car_center_meters = car_center_pixels*xm_per_pix \n",
    "        #Car center with respect to lane center in meters\n",
    "        car_relative_position = lane_center_meters-car_center_meters\n",
    "        #print(car_relative_position, 'car_relative_position meters')\n",
    "        \n",
    "        left_fit_cr = np.polyfit(lefty * ym_per_pix, leftx * xm_per_pix, 2)\n",
    "        right_fit_cr = np.polyfit(righty * ym_per_pix, rightx * xm_per_pix, 2)\n",
    "\n",
    "        left_curverad = ((1 + (2 * left_fit_cr[0] * y_eval + left_fit_cr[1]) ** 2) ** 1.5) \\\n",
    "                        / np.absolute(2 * left_fit_cr[0])\n",
    "        right_curverad = ((1 + (2 * right_fit_cr[0] * y_eval + right_fit_cr[1]) ** 2) ** 1.5) \\\n",
    "                         / np.absolute(2 * right_fit_cr[0])\n",
    "        # Now our radius of curvature is in meters\n",
    "        #print('left curve rad: {}m    |     right curve rad: {}m'.format(left_curverad, right_curverad))\n",
    "\n",
    "        \n",
    "        # Create an image to draw the lines on\n",
    "        warp_zero = np.zeros_like(binary_warped).astype(np.uint8)\n",
    "        color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "\n",
    "        # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "        pts_left = np.array([np.transpose(np.vstack([fit_leftx, fity]))])\n",
    "        pts_right = np.array([np.flipud(np.transpose(np.vstack([fit_rightx, fity])))])\n",
    "        # pts_right = np.array([np.transpose(np.vstack([fit_rightx, fity]))])\n",
    "        pts = np.hstack((pts_left, pts_right))\n",
    "        \n",
    "        self.__left_line.add_measurement(fit_leftx, fity, left_curverad, 0).perform()\n",
    "        self.__right_line.add_measurement(fit_rightx, fity, right_curverad, 0).perform()\n",
    "\n",
    "\n",
    "        # Draw the lane onto the warped blank image\n",
    "        cv2.fillPoly(color_warp, np.int_([pts]), (0, 195, 255))\n",
    "\n",
    "        # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "        newwarp = cv2.warpPerspective(color_warp, warper_op['Minv'], (binary_warped.shape[1], binary_warped.shape[0]))\n",
    "\n",
    "        # Combine the result with the original image\n",
    "        result = cv2.addWeighted(undistorted, 1, newwarp, 0.5, 0)\n",
    "        \n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX    \n",
    "        cv2.putText(result, \"<--[   CURVE RADIUS   ]-->\", (535,710), font, 0.55, (255,255,255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(result, '{0:.2f}m'.format(left_curverad), (380,710), font, 0.75, (255,255,255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(result, '{0:.2f}m'.format(right_curverad), (850,710), font, 0.75, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        cv2.putText(result, \"Car Relative to Center\", (550,525), font, 0.55, (255,255,255), 2, cv2.LINE_AA)    \n",
    "        cv2.putText(result, str('{0:.2f}m'.format(car_relative_position)), (600,500), font, 0.95, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        \n",
    "        self.__save_image(cv2.cvtColor(result, cv2.COLOR_RGB2BGR), 'FINAL_LANE')\n",
    "\n",
    "        return self._apply_output(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Camera Calibration\n",
    "\n",
    "#### 1. Briefly state how you computed the camera matrix and distortion coefficients. Provide an example of a distortion corrected calibration image.\n",
    "\n",
    "I start by preparing \"object points\", which will be the (x, y, z) coordinates of the chessboard corners in the world. Here I am assuming the chessboard is fixed on the (x, y) plane at z=0, such that the object points are the same for each calibration image.  Thus, `objp` is just a replicated array of coordinates, and `objpoints` will be appended with a copy of it every time I successfully detect all chessboard corners in a test image.  `imgpoints` will be appended with the (x, y) pixel position of each of the corners in the image plane with each successful chessboard detection.  \n",
    "\n",
    "I then used the output `objpoints` and `imgpoints` to compute the camera calibration and distortion coefficients using the `cv2.calibrateCamera()` function.  I applied this distortion correction to the test image using the `cv2.undistort()` function and obtained this result: \n",
    "\n",
    "![calibration1.jpg](camera_cal/undistorted/calibration1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# base edges - doesn't work for all images in camera_cal directory (i.e., 1, 4, 5)\n",
    "calibration_images=glob.glob('camera_cal/calibration*.jpg')\n",
    "calibration_op = CameraCalibrationOp(\n",
    "    calibration_images=calibration_images, \n",
    "    x_inside_corners=9, \n",
    "    y_inside_corners=6\n",
    ").perform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Histogram of Oriented Gradients (HOG)\n",
    "\n",
    "#### 1. Explain how (and identify where in your code) you extracted HOG features from the training images.\n",
    "\n",
    "The code for this step is contained in the first code cell of the IPython notebook (or in lines # through # of the file called `some_file.py`).  \n",
    "\n",
    "I started by reading in all the `vehicle` and `non-vehicle` images.  Here is an example of one of each of the `vehicle` and `non-vehicle` classes:\n",
    "\n",
    "![alt text][image1]\n",
    "\n",
    "I then explored different color spaces and different `skimage.hog()` parameters (`orientations`, `pixels_per_cell`, and `cells_per_block`).  I grabbed random images from each of the two classes and displayed them to get a feel for what the `skimage.hog()` output looks like.\n",
    "\n",
    "Here is an example using the `YCrCb` color space and HOG parameters of `orientations=8`, `pixels_per_cell=(8, 8)` and `cells_per_block=(2, 2)`:\n",
    "\n",
    "\n",
    "![alt text][image2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Explain how you settled on your final choice of HOG parameters.\n",
    "\n",
    "I tried various combinations of parameters and..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Describe how (and identify where in your code) you trained a classifier using your selected HOG features (and color features if you used them).\n",
    "\n",
    "I trained a linear SVM using..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Sliding Window Search\n",
    "\n",
    "#### 1. Describe how (and identify where in your code) you implemented a sliding window search.  How did you decide what scales to search and how much to overlap windows?\n",
    "\n",
    "I decided to search random window positions at random scales all over the image and came up with this (ok just kidding I didn't actually ;):\n",
    "\n",
    "![alt text][image3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Show some examples of test images to demonstrate how your pipeline is working.  What did you do to optimize the performance of your classifier?\n",
    "\n",
    "Ultimately I searched on two scales using YCrCb 3-channel HOG features plus spatially binned color and histograms of color in the feature vector, which provided a nice result.  Here are some example images:\n",
    "\n",
    "![alt text][image4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "### Video Implementation\n",
    "\n",
    "#### 1. Provide a link to your final video output.  Your pipeline should perform reasonably well on the entire project video (somewhat wobbly or unstable bounding boxes are ok as long as you are identifying the vehicles most of the time with minimal false positives.)\n",
    "\n",
    "Here's a [link to my video result](./project_video.mp4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Describe how (and identify where in your code) you implemented some kind of filter for false positives and some method for combining overlapping bounding boxes.\n",
    "\n",
    "I recorded the positions of positive detections in each frame of the video.  From the positive detections I created a heatmap and then thresholded that map to identify vehicle positions.  I then used `scipy.ndimage.measurements.label()` to identify individual blobs in the heatmap.  I then assumed each blob corresponded to a vehicle.  I constructed bounding boxes to cover the area of each blob detected.  \n",
    "\n",
    "Here's an example result showing the heatmap from a series of frames of video, the result of `scipy.ndimage.measurements.label()` and the bounding boxes then overlaid on the last frame of video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are six frames and their corresponding heatmaps:\n",
    "\n",
    "![alt text][image5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is the output of `scipy.ndimage.measurements.label()` on the integrated heatmap from all six frames:\n",
    "![alt text][image6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here the resulting bounding boxes are drawn onto the last frame in the series:\n",
    "![alt text][image7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Discussion\n",
    "\n",
    "#### 1. Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?\n",
    "\n",
    "Here I'll talk about the approach I took, what techniques I used, what worked and why, where the pipeline might fail and how I might improve it if I were going to pursue this project further. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:carnd]",
   "language": "python",
   "name": "conda-env-carnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
