{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 5 - Vehicle Detection and Tracking\n",
    "\n",
    "##### By Matthew Zimmer - Future Self-Driving Car Engineer\n",
    "\n",
    "[GitHub](https://github.com/matthewzimmer) | [LinkedIn](https://www.linkedin.com/in/matthewazimmer)\n",
    "\n",
    "---\n",
    "\n",
    "**Vehicle Detection Project**\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images and train a classifier Linear SVM classifier\n",
    "* Optionally, you can also apply a color transform and append binned color features, as well as histograms of color, to your HOG feature vector. \n",
    "* Note: for those first two steps don't forget to normalize your features and randomize a selection for training and testing.\n",
    "* Implement a sliding-window technique and use your trained classifier to search for vehicles in images.\n",
    "* Run your pipeline on a video stream (start with the test_video.mp4 and later implement on full project_video.mp4) and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles.\n",
    "* Estimate a bounding box for vehicles detected.\n",
    "\n",
    "[//]: # (Image References)\n",
    "[image1]: ./examples/car_not_car.png\n",
    "[image2]: ./examples/HOG_example.jpg\n",
    "[image3]: ./examples/sliding_windows.jpg\n",
    "[image4]: ./examples/sliding_window.jpg\n",
    "[image5]: ./examples/bboxes_and_heat.png\n",
    "[image6]: ./examples/labels_map.png\n",
    "[image7]: ./examples/output_bboxes.png\n",
    "[video1]: ./project_video.mp4\n",
    "\n",
    "## [Rubric](https://review.udacity.com/#!/rubrics/513/view) Points\n",
    "\n",
    "### Here I will consider the rubric points individually and describe how I addressed each point in my implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Writeup / README\n",
    "\n",
    "#### 1. Provide a Writeup / README that includes all the rubric points and how you addressed each one.  You can submit your writeup as markdown or pdf.  [Here](https://github.com/udacity/CarND-Vehicle-Detection/blob/master/writeup_template.md) is a template writeup for this project you can use as a guide and a starting point.  \n",
    "\n",
    "You're reading it!\n",
    "\n",
    "> **CRITICAL NOTE** I treated this notebook as a comprehensive tutorial walking you from start to finish, top down, starting from calibrating the camera all the way to the very last step of drawing the detected lane region into the road. The very last cell of this notebook offers you the ability to control the hyper parameters used by my pipeline and either test them on a single image (the default setting) or test the sample the entire video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('qt5agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.ndimage.measurements import label\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "from skimage.feature import hog\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from IPython.core.display import Image, display\n",
    "from IPython.display import YouTubeVideo\n",
    "def render_youtube_video(video_id, width=880, height=495):\n",
    "    return YouTubeVideo(video_id, width=width, height=height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Pipeline Operations\n",
    "\n",
    "Pipeline operations are the principle driving force for this project. Each implementation of `PipelineOp` is a modular, reusable algorithm which, in its most basic form, performs a single operation on an image.\n",
    "\n",
    "**[PipelineOp](https://github.com/matthewzimmer/CarND-Vehicle-Detection-P5/blob/107239a1530cc3b1f56016483941b28e5b70c26a/lib/pipeline_ops.py#L10-L39)** has a simple interface with only 2 requirements to satisfy the contract:\n",
    "\n",
    "1. Declare a constructor with inputs necessary to perform the operation. To truly adhere to the nature of encapsulation and immutability, I initialize private variables so as not to expose them publicly.\n",
    "\n",
    "2. Implement your **[#perform](https://github.com/matthewzimmer/CarND-Vehicle-Detection-P5/blob/107239a1530cc3b1f56016483941b28e5b70c26a/lib/pipeline_ops.py#L31-L32)** method\n",
    "\n",
    "    * Your implementation must `return self`. This provides support to perform the op and immediately assign a chained call to `#output` to a local variable. Example:\n",
    "    \n",
    "    ```python\n",
    "    out = PipelineOp().output()\n",
    "    ```\n",
    "\n",
    "    * Declare your op's final output by calling **[#_apply_output](https://github.com/matthewzimmer/CarND-Vehicle-Detection-P5/blob/107239a1530cc3b1f56016483941b28e5b70c26a/lib/pipeline_ops.py#L37-L39)** once you've performed your operation. Note the value and data type of output is arbitraily defined by your operation. Documenting this information is encouraged.\n",
    "    \n",
    "This architecture provides flexibility to implementing more complicated algorithms that have many moving parts while still adhering to the contract by producing a single arbitrary output object (e.g., Dictionary, Image, Array). I demonstrate a healthy mixture of both simple and complex **[PipelineOp](https://github.com/matthewzimmer/CarND-Vehicle-Detection-P5/blob/107239a1530cc3b1f56016483941b28e5b70c26a/lib/pipeline_ops.py#L10-L39)** implementations in this project. For example, **[CameraCalibrationOp](https://github.com/matthewzimmer/CarND-Vehicle-Detection-P5/blob/107239a1530cc3b1f56016483941b28e5b70c26a/lib/lane_detection_ops.py#L12-L230)** and **[LaneAssistOp](https://github.com/matthewzimmer/CarND-Vehicle-Detection-P5/blob/107239a1530cc3b1f56016483941b28e5b70c26a/lib/lane_detection_ops.py#L402-L677)** are great examples of a complex algorithm whereas **[ColorThreshOp](https://github.com/matthewzimmer/CarND-Vehicle-Detection-P5/blob/107239a1530cc3b1f56016483941b28e5b70c26a/lib/pipeline_ops.py#L63-L72)** is a great example of a minimalistic algorithm, both adhering to the same **PipelineOp** contract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lib.pipeline_ops import *\n",
    "from lib.vehicle_detection_ops import *\n",
    "from lib.lane_detection_ops import *\n",
    "from lib.datasets import *\n",
    "\n",
    "calibration_op = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a function to return HOG features and visualization\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, \n",
    "                        vis=False, feature_vec=True, transform_sqrt=True):\n",
    "    # Call with two outputs if vis==True\n",
    "#     if vis == True:\n",
    "#         features, hog_image = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "#                                   cells_per_block=(cell_per_block, cell_per_block), transform_sqrt=True, \n",
    "#                                   visualise=vis, feature_vector=feature_vec)\n",
    "#         return features, hog_image\n",
    "#     # Otherwise call with one output\n",
    "#     else:      \n",
    "#         features = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "#                        cells_per_block=(cell_per_block, cell_per_block), transform_sqrt=True, \n",
    "#                        visualise=vis, feature_vector=feature_vec)\n",
    "#     return features\n",
    "    return HOGExtractorOp(\n",
    "        img, \n",
    "        orient=orient, \n",
    "        pix_per_cell=pix_per_cell, \n",
    "        cell_per_block=cell_per_block, \n",
    "        visualize=vis, \n",
    "        feature_vec=feature_vec, \n",
    "        transform_sqrt=transform_sqrt\n",
    "    ).output()[0]\n",
    "\n",
    "# Define a function to extract features from a list of images\n",
    "# Have this function call bin_spatial() and color_hist()\n",
    "def extract_features(imgs, color_space='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, orient=9, \n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0,\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True):\n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    # Iterate through the list of images\n",
    "    for file in imgs:\n",
    "        file_features = []\n",
    "        # Read in each one by one\n",
    "        image = mpimg.imread(file)\n",
    "        # apply color conversion if other than 'RGB'\n",
    "        if color_space != 'RGB':\n",
    "            if color_space == 'HSV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "            elif color_space == 'LUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\n",
    "            elif color_space == 'HLS':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "            elif color_space == 'YUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "            elif color_space == 'YCrCb':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n",
    "        else: feature_image = np.copy(image)      \n",
    "\n",
    "        if spatial_feat == True:\n",
    "            spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "            file_features.append(spatial_features)\n",
    "        if hist_feat == True:\n",
    "            # Apply color_hist()\n",
    "            hist_features = color_hist(feature_image, nbins=hist_bins)\n",
    "            file_features.append(hist_features)\n",
    "        if hog_feat == True:\n",
    "        # Call get_hog_features() with vis=False, feature_vec=True\n",
    "            if hog_channel == 'ALL':\n",
    "                hog_features = []\n",
    "                for channel in range(feature_image.shape[2]):\n",
    "                    hog_features.append(get_hog_features(feature_image[:,:,channel], \n",
    "                                        orient, pix_per_cell, cell_per_block, \n",
    "                                        vis=False, feature_vec=True))\n",
    "                hog_features = np.ravel(hog_features)        \n",
    "            else:\n",
    "                hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                            pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "            # Append the new feature vector to the features list\n",
    "            file_features.append(hog_features)\n",
    "        features.append(np.concatenate(file_features))\n",
    "    # Return list of feature vectors\n",
    "    return features\n",
    "\n",
    "# Define a function to compute binned color features  \n",
    "def bin_spatial(img, size=(32, 32)):\n",
    "    # Use cv2.resize().ravel() to create the feature vector\n",
    "    features = cv2.resize(img, size).ravel() \n",
    "    # Return the feature vector\n",
    "    return features\n",
    "\n",
    "# Define a function to compute color histogram features  \n",
    "def color_hist(img, nbins=32):\n",
    "    # Compute the histogram of the color channels separately\n",
    "    channel1_hist = np.histogram(img[:,:,0], bins=nbins)\n",
    "    channel2_hist = np.histogram(img[:,:,1], bins=nbins)\n",
    "    channel3_hist = np.histogram(img[:,:,2], bins=nbins)\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "    # Return the individual histograms, bin_centers and feature vector\n",
    "    return hist_features\n",
    "\n",
    "# Define a function that takes an image,\n",
    "# start and stop positions in both x and y, \n",
    "# window size (x and y dimensions),  \n",
    "# and overlap fraction (for both x and y)\n",
    "def slide_window(img, x_start_stop=[None, None], y_start_stop=[None, None], \n",
    "                    xy_window=(64, 64), xy_overlap=(0.5, 0.5)):\n",
    "    # If x and/or y start/stop positions not defined, set to image size\n",
    "    if x_start_stop[0] == None:\n",
    "        x_start_stop[0] = 0\n",
    "    if x_start_stop[1] == None:\n",
    "        x_start_stop[1] = img.shape[1]\n",
    "    if y_start_stop[0] == None:\n",
    "        y_start_stop[0] = 0\n",
    "    if y_start_stop[1] == None:\n",
    "        y_start_stop[1] = img.shape[0]\n",
    "    # Compute the span of the region to be searched    \n",
    "    xspan = x_start_stop[1] - x_start_stop[0]\n",
    "    yspan = y_start_stop[1] - y_start_stop[0]\n",
    "    # Compute the number of pixels per step in x/y\n",
    "    nx_pix_per_step = np.int(xy_window[0]*(1 - xy_overlap[0]))\n",
    "    ny_pix_per_step = np.int(xy_window[1]*(1 - xy_overlap[1]))\n",
    "    # Compute the number of windows in x/y\n",
    "    nx_windows = np.int(xspan/nx_pix_per_step) - 1\n",
    "    ny_windows = np.int(yspan/ny_pix_per_step) - 1\n",
    "    # Initialize a list to append window positions to\n",
    "    window_list = []\n",
    "    # Loop through finding x and y window positions\n",
    "    # Note: you could vectorize this step, but in practice\n",
    "    # you'll be considering windows one by one with your\n",
    "    # classifier, so looping makes sense\n",
    "    for ys in range(ny_windows):\n",
    "        for xs in range(nx_windows):\n",
    "            # Calculate window position\n",
    "            startx = xs*nx_pix_per_step + x_start_stop[0]\n",
    "            endx = startx + xy_window[0]\n",
    "            starty = ys*ny_pix_per_step + y_start_stop[0]\n",
    "            endy = starty + xy_window[1]\n",
    "            # Append window position to list\n",
    "            window_list.append(((startx, starty), (endx, endy)))\n",
    "    # Return the list of windows\n",
    "    return window_list\n",
    "\n",
    "# Define a function to draw bounding boxes\n",
    "def draw_boxes(img, bboxes, color=(0, 0, 255), thick=6):\n",
    "    # Make a copy of the image\n",
    "    imcopy = np.copy(img)\n",
    "    # Iterate through the bounding boxes\n",
    "    for bbox in bboxes:\n",
    "        # Draw a rectangle given bbox coordinates\n",
    "        cv2.rectangle(imcopy, bbox[0], bbox[1], color, thick)\n",
    "    # Return the image copy with boxes drawn\n",
    "    return imcopy\n",
    "\n",
    "# Define a function to extract features from a single image window\n",
    "# This function is very similar to extract_features()\n",
    "# just for a single image rather than list of images\n",
    "def single_img_features(img, color_space='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, orient=9, \n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0,\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True):    \n",
    "    #1) Define an empty list to receive features\n",
    "    img_features = []\n",
    "    #2) Apply color conversion if other than 'RGB'\n",
    "    if color_space != 'RGB':\n",
    "        if color_space == 'HSV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "        elif color_space == 'LUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2LUV)\n",
    "        elif color_space == 'HLS':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "        elif color_space == 'YUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "        elif color_space == 'YCrCb':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n",
    "    else: feature_image = np.copy(img)      \n",
    "    #3) Compute spatial features if flag is set\n",
    "    if spatial_feat == True:\n",
    "        spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "        #4) Append features to list\n",
    "        img_features.append(spatial_features)\n",
    "    #5) Compute histogram features if flag is set\n",
    "    if hist_feat == True:\n",
    "        hist_features = color_hist(feature_image, nbins=hist_bins)\n",
    "        #6) Append features to list\n",
    "        img_features.append(hist_features)\n",
    "    #7) Compute HOG features if flag is set\n",
    "    if hog_feat == True:\n",
    "        if hog_channel == 'ALL':\n",
    "            hog_features = []\n",
    "            for channel in range(feature_image.shape[2]):\n",
    "                hog_features.extend(get_hog_features(feature_image[:,:,channel], \n",
    "                                    orient, pix_per_cell, cell_per_block, \n",
    "                                    vis=False, feature_vec=True))      \n",
    "        else:\n",
    "            hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                        pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "        #8) Append features to list\n",
    "        img_features.append(hog_features)\n",
    "\n",
    "    #9) Return concatenated array of features\n",
    "    return np.concatenate(img_features)\n",
    "\n",
    "def draw_labeled_bboxes(img, labels):\n",
    "    # Iterate through all detected cars\n",
    "    for car_number in range(1, labels[1]+1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        # Draw the box on the image\n",
    "        cv2.rectangle(img, bbox[0], bbox[1], (0,0,255), 6)\n",
    "    # Return the image\n",
    "    return img\n",
    "\n",
    "def plot_histogram(image, nbins=32, bins_range=(0, 256), title=''):\n",
    "    ch1h, ch2h, ch3h, bincen, feature_vec = ColorHistOp(image, nbins=nbins, bins_range=bins_range).perform().output()\n",
    "\n",
    "    # Plot a figure with all three bar charts\n",
    "    if ch1h is not None:\n",
    "        fig = plt.figure(figsize=(12,3))\n",
    "        plt.subplot(131)\n",
    "        plt.bar(bincen, ch1h[0])\n",
    "        plt.xlim(0, 256)\n",
    "        plt.title(title+' Ch1 Histogram')\n",
    "        plt.subplot(132)\n",
    "        plt.bar(bincen, ch2h[0])\n",
    "        plt.xlim(0, 256)\n",
    "        plt.title(title+' Ch2 Histogram')\n",
    "        plt.subplot(133)\n",
    "        plt.bar(bincen, ch3h[0])\n",
    "        plt.xlim(0, 256)\n",
    "        plt.title(title+' Ch3 Histogram')\n",
    "        fig.tight_layout()\n",
    "    else:\n",
    "        print('Your function is returning None for at least one variable...')\n",
    "\n",
    "def add_heat(heatmap, bbox_list):\n",
    "    # Iterate through list of bboxes\n",
    "    for box in bbox_list:\n",
    "        # Add += 1 for all pixels inside each bbox\n",
    "        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\n",
    "    \n",
    "    # Return updated heatmap\n",
    "    return heatmap\n",
    "    \n",
    "def apply_threshold(heatmap, threshold):\n",
    "    # Zero out pixels below the threshold\n",
    "    heatmap[heatmap <= threshold] = 0\n",
    "    # Return thresholded map\n",
    "    return heatmap\n",
    "\n",
    "class Params():\n",
    "    def __init__(\n",
    "        self, \n",
    "        colorspace='YCrBr',\n",
    "        orient=9,\n",
    "        pix_per_cell=4, \n",
    "        cell_per_block=4, \n",
    "        hog_channel='ALL',\n",
    "        spatial_size=(64, 64),\n",
    "        hist_bins=64,\n",
    "        spatial_feat=True,\n",
    "        hist_feat=True,\n",
    "        hog_feat=True\n",
    "    ):\n",
    "        self.colorspace = colorspace # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "        self.orient = orient # typically between 6 and 12\n",
    "        self.pix_per_cell = pix_per_cell # HOG pixels per cell\n",
    "        self.cell_per_block = cell_per_block # HOG cells per block\n",
    "        self.hog_channel = hog_channel # Can be 0, 1, 2, or \"ALL\"\n",
    "        self.spatial_size = spatial_size # Spatial binning dimensions\n",
    "        self.hist_bins = hist_bins # Number of histogram bins\n",
    "        self.spatial_feat = spatial_feat # Spatial features on or off\n",
    "        self.hist_feat = hist_feat # Histogram features on or off\n",
    "        self.hog_feat = hog_feat  # HOG features on or off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Camera Calibration\n",
    "\n",
    "My entire camera calibration algorithm may be found inside of **[CameraCalibrationOp#perform](https://github.com/matthewzimmer/CarND-Vehicle-Detection-P5/blob/107239a1530cc3b1f56016483941b28e5b70c26a/lib/lane_detection_ops.py#L54-L68)**. I've also exposed the **[#undistort](https://github.com/matthewzimmer/CarND-Vehicle-Detection-P5/blob/107239a1530cc3b1f56016483941b28e5b70c26a/lib/lane_detection_ops.py#L70-L83)** method which will undistort any raw image (ideally images taken by that camera but no code was put in place to validate camera source of image though EXIF data would be perfect place to look first).\n",
    "\n",
    "I start by preparing \"object points\", which will be the (x, y, z) coordinates of the chessboard corners in the world. Here I am assuming the chessboard is fixed on the (x, y) plane at z=0, such that the object points are the same for each calibration image.  Thus, `objp` is just a replicated array of coordinates, and `objpoints` will be appended with a copy of it every time I successfully detect all chessboard corners in a test image.  `imgpoints` will be appended with the (x, y) pixel position of each of the corners in the image plane with each successful chessboard detection.  \n",
    "\n",
    "I then used the output `objpoints` and `imgpoints` to compute the camera calibration and distortion coefficients using the `cv2.calibrateCamera()` function.  I applied this distortion correction to the test image using the `cv2.undistort()` function and obtained this result: \n",
    "\n",
    "![calibration1.jpg](camera_cal/undistorted/calibration1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calibrate_camera():\n",
    "    global calibration_op\n",
    "    \n",
    "    if calibration_op == None:\n",
    "        # base edges - doesn't work for all images in camera_cal directory (i.e., 1, 4, 5)\n",
    "        calibration_images=glob.glob('camera_cal/calibration*.jpg')\n",
    "\n",
    "        # I will now inject this calibration_op instance later on \n",
    "        # into my pipeline principally used to undistort the \n",
    "        # raw image.\n",
    "        calibration_op = CameraCalibrationOp(\n",
    "            calibration_images=calibration_images, \n",
    "            x_inside_corners=9, \n",
    "            y_inside_corners=6\n",
    "        ).perform()\n",
    "    return calibration_op\n",
    "calibrate_camera()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Histogram of Oriented Gradients (HOG)\n",
    "\n",
    "#### 1. Explain how (and identify where in your code) you extracted HOG features from the training images.\n",
    "\n",
    "The code for all steps from here on out is contained in the **Pipeline Operations** code cell at the beginning of the IPython notebook.\n",
    "\n",
    "I started by reading in all the `vehicle` and `non-vehicle` images.  Here is an example of one of each of the `vehicle` and `non-vehicle` classes:\n",
    "\n",
    "![alt text][image1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = CarsNotCarsDatasetOp(dataset_size='small').perform()\n",
    "cars = ds.cars()\n",
    "notcars = ds.notcars()\n",
    "\n",
    "print('    # Cars: ' + str(len(cars)))\n",
    "print('# Not cars: ' + str(len(notcars)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of one of each of the `vehicle` and `non-vehicle` classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a random index to look at a car image\n",
    "ind_cars = np.random.randint(0, len(cars))\n",
    "c_image = mpimg.imread(cars[ind_cars])\n",
    "\n",
    "\n",
    "plt.title(\"CAR - {} - {}\".format(ind_cars,cars[ind_cars]))\n",
    "plt.subplot(121)\n",
    "plt.imshow(c_image)\n",
    "plt.show()\n",
    "\n",
    "# Generate a random index to look at a notcar image\n",
    "ind_notcars = np.random.randint(0, len(notcars))\n",
    "nc_image = mpimg.imread(notcars[ind_notcars])\n",
    "\n",
    "plt.title(\"NOTCAR - {} - {}\".format(ind_notcars, notcars[ind_notcars]))\n",
    "plt.subplot(122)\n",
    "plt.imshow(nc_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_histogram(c_image, title='[C][RGB]')\n",
    "plot_histogram(cv2.cvtColor(c_image, cv2.COLOR_RGB2YCrCb), title='[C][YCrCb]')\n",
    "plot_histogram(cv2.cvtColor(c_image, cv2.COLOR_RGB2YUV), title='[C][YUV]')\n",
    "plot_histogram(cv2.cvtColor(c_image, cv2.COLOR_RGB2LUV), title='[C][LUV]')\n",
    "plot_histogram(cv2.cvtColor(c_image, cv2.COLOR_RGB2HLS), title='[C][HLS]')\n",
    "plot_histogram(cv2.cvtColor(c_image, cv2.COLOR_RGB2HSV), title='[C][HSV]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_histogram(nc_image, title='[C][RGB]')\n",
    "plot_histogram(cv2.cvtColor(nc_image, cv2.COLOR_RGB2YCrCb), title='[C][YCrCb]')\n",
    "plot_histogram(cv2.cvtColor(nc_image, cv2.COLOR_RGB2YUV), title='[C][YUV]')\n",
    "plot_histogram(cv2.cvtColor(nc_image, cv2.COLOR_RGB2LUV), title='[C][LUV]')\n",
    "plot_histogram(cv2.cvtColor(nc_image, cv2.COLOR_RGB2HLS), title='[C][HLS]')\n",
    "plot_histogram(cv2.cvtColor(nc_image, cv2.COLOR_RGB2HSV), title='[C][HSV]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then explored different color spaces and different `skimage.hog()` parameters (`orientations`, `pixels_per_cell`, and `cells_per_block`).  I grabbed random images from each of the two classes and displayed them to get a feel for what the `skimage.hog()` output looks like.\n",
    "\n",
    "Here is an example using the `YCrCb` color space and HOG parameters of `orientations=8`, `pixels_per_cell=(8, 8)` and `cells_per_block=(2, 2)`:\n",
    "\n",
    "\n",
    "![alt text][image2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_and_visualize(orig, gray, orient=8, pix_per_cell=8, cell_per_block=2, transform_sqrt=False):\n",
    "    # Call our function with vis=True to see an image output\n",
    "    features, hog_image = HOGExtractorOp(\n",
    "        gray, orient, \n",
    "        pix_per_cell, cell_per_block, \n",
    "        visualize=True,\n",
    "        feature_vec=False, \n",
    "        transform_sqrt=transform_sqrt\n",
    "    ).output()\n",
    "\n",
    "    # Plot the examples\n",
    "    fig = plt.figure()\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(orig)\n",
    "    plt.title('Orig. - {}'.format(str(ind_cars)))\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(gray, cmap='gray')\n",
    "    plt.title('Gray - {}'.format(str(ind_cars)))\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(hog_image, cmap='hot')\n",
    "    plt.title('HOG Vis. {}'.format(str(ind_cars)))\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "# c_ind_cars = np.random.randint(0, len(cars))\n",
    "# nc_ind_cars = np.random.randint(0, len(notcars))\n",
    "\n",
    "# Read in the image\n",
    "# ind_cars = 1165\n",
    "#c_image = mpimg.imread(cars[c_ind_cars])\n",
    "#nc_image = mpimg.imread(notcars[nc_ind_cars])\n",
    "\n",
    "for i in range(3):\n",
    "    cspace = 'YCrCb'\n",
    "#     cspace = 'YUV'\n",
    "#     cspace = 'HSV'\n",
    "#     cspace = 'HLS'\n",
    "    c_gray = cv2.cvtColor(c_image, eval('cv2.COLOR_RGB2'+cspace))[:,:,i]\n",
    "    nc_gray = cv2.cvtColor(nc_image, eval('cv2.COLOR_RGB2'+cspace))[:,:,i]\n",
    "\n",
    "\n",
    "    # Define HOG parameters\n",
    "    orient = 7\n",
    "    pix_per_cell = 6\n",
    "    cell_per_block = 4\n",
    "\n",
    "\n",
    "    extract_and_visualize(c_image, c_gray, orient=orient, pix_per_cell=pix_per_cell, cell_per_block=cell_per_block, transform_sqrt=True)\n",
    "    #extract_and_visualize(c_image, c_gray, orient=orient, pix_per_cell=pix_per_cell, cell_per_block=cell_per_block, transform_sqrt=False)\n",
    "    \n",
    "    #extract_and_visualize(nc_image, nc_gray, orient=orient, pix_per_cell=pix_per_cell, cell_per_block=cell_per_block, transform_sqrt=True)\n",
    "    #extract_and_visualize(nc_image, nc_gray, orient=orient, pix_per_cell=pix_per_cell, cell_per_block=cell_per_block, transform_sqrt=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Explain how you settled on your final choice of HOG parameters.\n",
    "\n",
    "I tried various combinations of parameters and the one with the highest accuracy is ultimately what I chose for my model.\n",
    "\n",
    "Each item outlined in the table below is initiatlized and commented out in the cell below. Feel free to have a go at it.\n",
    "\n",
    "\n",
    "|  colorspace | orient | pix_per_cell | cell_per_block | hog_channel  |   accuracy   |\n",
    "|:--------:|:------------:|:------------:|:------------:|:------------:|:------------:|\n",
    "| YUV | 9 | 8 | 2 | 0 | 97% |\n",
    "| YUV | 8 | 8 | 2 | 0 | 98% |\n",
    "| YUV | 8 | 8 | 2 | ALL | 98.5% |\n",
    "| YUV | 8 | 7 | 2 | ALL | 99% |\n",
    "| YCrCb | 8 | 7 | 2 | ALL | 99-100% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Describe how (and identify where in your code) you trained a classifier using your selected HOG features (and color features if you used them).\n",
    "\n",
    "I trained a linear SVM using...\n",
    "\n",
    "1. I instantiate `Params` with the HOG parameters that yielded the best overall prediction accuracy against the test set.\n",
    "\n",
    "2. I then invoke `#extract_features` which extracts HOG features from each training image for both the cars and not cars datasets passing the same parameter to each.\n",
    "\n",
    "3. I then stack both cars and not cars datasets into a single array called `X` which correspond to my training features.\n",
    "\n",
    "4. Using sklearn.preprocessing.StandardScaler(), I normalize my feature vectors for training my classifier. \n",
    "\n",
    "5. Then I apply the same scaling to each of the feature vectors.\n",
    "\n",
    "6. Next, I created my training labels by using np.hstack which assigns the label `1` for each item from the `cars` training set and the label `0` for each imem in the `notcars` training set.\n",
    "\n",
    "7. Then I split up the data into randomized 80% training and 20% test sets using `sklearn.model_selection.train_test_split`. This automatically shuffles my dataset.\n",
    "\n",
    "8. Using `sklearn.svm.LinearSVC`, I fit my training features and labels to the model.\n",
    "\n",
    "9. Finally, I run a prediction against my model and print some statistics to the console below the next Jupyter Notebook cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "# params = Params(colorspace='YUV', orient=9, pix_per_cell=8, cell_per_block=2, hog_channel=0) #97%\n",
    "# params = Params(colorspace='YUV', orient=8, pix_per_cell=8, cell_per_block=2, hog_channel=0) #98%\n",
    "# params = Params(colorspace='YUV', orient=8, pix_per_cell=8, cell_per_block=2, hog_channel='ALL') #98.5%\n",
    "# params = Params(colorspace='YUV', orient=8, pix_per_cell=7, cell_per_block=2, hog_channel='ALL') #99%\n",
    "# params = Params(colorspace='YCrCb', orient=8, pix_per_cell=7, cell_per_block=2, hog_channel='ALL') #99-100%\n",
    "# params = Params(colorspace='YCrCb', orient=7, pix_per_cell=4, cell_per_block=2, hog_channel='ALL') #99-100%\n",
    "# params = Params(colorspace='YCrCb', orient=9, pix_per_cell=4, cell_per_block=4, hog_channel='ALL') #99-100%\n",
    "params = Params(colorspace='YCrCb', orient=7, pix_per_cell=8, cell_per_block=2, hog_channel='ALL') #99-100%\n",
    "################################################################################################################\n",
    "\n",
    "t=time.time()\n",
    "car_features = extract_features(cars, color_space=params.colorspace, orient=params.orient, \n",
    "                        pix_per_cell=params.pix_per_cell, cell_per_block=params.cell_per_block, \n",
    "                        hog_channel=params.hog_channel)\n",
    "notcar_features = extract_features(notcars, color_space=params.colorspace, orient=params.orient, \n",
    "                        pix_per_cell=params.pix_per_cell, cell_per_block=params.cell_per_block, \n",
    "                        hog_channel=params.hog_channel)\n",
    "\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 2), 'Seconds to extract HOG features...')\n",
    "\n",
    "# Create an array stack of feature vectors\n",
    "X = np.vstack((car_features, notcar_features)).astype(np.float64)                        \n",
    "# Fit a per-column scaler\n",
    "X_scaler = StandardScaler().fit(X)\n",
    "# Apply the scaler to X\n",
    "scaled_X = X_scaler.transform(X)\n",
    "# Define the labels vector\n",
    "y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\n",
    "# Split up data into randomized training and test sets\n",
    "rand_state = np.random.randint(0, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    scaled_X, y, test_size=0.2, random_state=rand_state)\n",
    "\n",
    "print('Using:',params.orient,'orientations',params.pix_per_cell,\n",
    "    'pixels per cell and', params.cell_per_block,'cells per block')\n",
    "print('Feature vector length:', len(X_train[0]))\n",
    "\n",
    "# Use a linear SVC \n",
    "svc = LinearSVC(C=1.)\n",
    "# Check the training time for the SVC\n",
    "t=time.time()\n",
    "svc.fit(X_train, y_train)\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "# Check the score of the SVC\n",
    "print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))\n",
    "# Check the prediction time for a single sample\n",
    "t=time.time()\n",
    "n_predict = len(X_test)\n",
    "print('My SVC predicts: ', svc.predict(X_test[0:n_predict]))\n",
    "print('For these',n_predict, 'labels: ', y_test[0:n_predict])\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 5), 'Seconds to predict', n_predict,'labels with SVC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Sliding Window Search\n",
    "\n",
    "#### 1. Describe how (and identify where in your code) you implemented a sliding window search.  How did you decide what scales to search and how much to overlap windows?\n",
    "\n",
    "I decided to search random window positions at random scales all over the image and came up with this (ok just kidding I didn't actually ;):\n",
    "\n",
    "![alt text][image3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = CarsNotCarsDatasetOp(dataset_size='big').perform()\n",
    "cars = ds.cars()\n",
    "notcars = ds.notcars()\n",
    "\n",
    "# Reduce the sample size because\n",
    "# The quiz evaluator times out after 13s of CPU time\n",
    "sample_size = 1000\n",
    "c_random_idxs = np.random.randint(0,len(cars), sample_size)\n",
    "nc_random_idxs = np.random.randint(0,len(notcars), sample_size)\n",
    "test_cars = cars #np.array(cars)[c_random_idxs] # cars[0:c_sample_size]\n",
    "test_notcars = notcars #np.array(notcars)[nc_random_idxs] # notcars[0:nc_sample_size]\n",
    "\n",
    "print('    # Cars: ' + str(len(test_cars)))\n",
    "print('# Not cars: ' + str(len(test_notcars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_classifier(params, test_cars, test_notcars, C=1.):\n",
    "    t=time.time()\n",
    "    car_features = extract_features(test_cars, color_space=params.colorspace, \n",
    "                            spatial_size=params.spatial_size, hist_bins=params.hist_bins, \n",
    "                            orient=params.orient, pix_per_cell=params.pix_per_cell, \n",
    "                            cell_per_block=params.cell_per_block, \n",
    "                            hog_channel=params.hog_channel, spatial_feat=params.spatial_feat, \n",
    "                            hist_feat=params.hist_feat, hog_feat=params.hog_feat)\n",
    "    \n",
    "    notcar_features = extract_features(test_notcars, color_space=params.colorspace, \n",
    "                            spatial_size=params.spatial_size, hist_bins=params.hist_bins, \n",
    "                            orient=params.orient, pix_per_cell=params.pix_per_cell, \n",
    "                            cell_per_block=params.cell_per_block, \n",
    "                            hog_channel=params.hog_channel, spatial_feat=params.spatial_feat, \n",
    "                            hist_feat=params.hist_feat, hog_feat=params.hog_feat)\n",
    "    t2 = time.time()\n",
    "    print(round(t2-t, 2), 'Seconds to extract HOG features...')\n",
    "\n",
    "    X = np.vstack((car_features, notcar_features)).astype(np.float64)                        \n",
    "\n",
    "    # Define the labels vector\n",
    "    y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\n",
    "\n",
    "    # Fit a per-column scaler\n",
    "    X_scaler = StandardScaler().fit(X)\n",
    "    # Apply the scaler to X\n",
    "    scaled_X = X_scaler.transform(X)\n",
    "    \n",
    "    # Split up data into randomized training and test sets\n",
    "    rand_state = np.random.randint(0, 100)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.2, random_state=rand_state)\n",
    "\n",
    "    print('Using:',params.orient,'orientations',params.pix_per_cell,\n",
    "        'pixels per cell and', params.cell_per_block,'cells per block')\n",
    "    print('Feature vector length:', len(X_train[0]))\n",
    "    # Use a linear SVC \n",
    "    # svc = LinearSVC(C=1.2) # 98.76%\n",
    "    svc = LinearSVC(C=C)\n",
    "    # from sklearn.ensemble import AdaBoostClassifier\n",
    "    # svc = AdaBoostClassifier(learning_rate=0.1, algorithm='SAMME.R', n_estimators=50)\n",
    "    # Check the training time for the SVC\n",
    "    t=time.time()\n",
    "    svc.fit(X_train, y_train)\n",
    "    t2 = time.time()\n",
    "    print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "    # Check the score of the SVC\n",
    "    print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))\n",
    "    \n",
    "    return svc, X_scaler\n",
    "\n",
    "# params = Params(\n",
    "#     colorspace='YCrCb', \n",
    "#     orient=7, \n",
    "#     pix_per_cell=8, \n",
    "#     cell_per_block=2, \n",
    "#     hog_channel='ALL', \n",
    "#     spatial_size=(32, 32), \n",
    "#     hist_bins=32,\n",
    "#     spatial_feat=True,\n",
    "#     hist_feat=True,\n",
    "#     hog_feat=True,\n",
    "#     y_start_stop=[400, 656]\n",
    "# ) #98.37%\n",
    "\n",
    "params = Params(\n",
    "    colorspace='YCrCb', \n",
    "    orient=7,\n",
    "    pix_per_cell=8, \n",
    "    cell_per_block=2, \n",
    "    hog_channel='ALL', \n",
    "    spatial_size=(32, 32), \n",
    "    hist_bins=32,\n",
    "    spatial_feat=True,\n",
    "    hist_feat=True,\n",
    "    hog_feat=True\n",
    ") #99.21%\n",
    "\n",
    "svc, X_scaler = train_classifier(params, test_cars, test_notcars, C=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a function you will pass an image \n",
    "# and the list of windows to be searched (output of slide_windows())\n",
    "def search_windows(img, windows, clf, scaler, color_space='RGB', \n",
    "                    spatial_size=(32, 32), hist_bins=32, \n",
    "                    hist_range=(0, 256), orient=9, \n",
    "                    pix_per_cell=8, cell_per_block=2, \n",
    "                    hog_channel=0, spatial_feat=True, \n",
    "                    hist_feat=True, hog_feat=True):\n",
    "    X_pred = []\n",
    "    \n",
    "    #1) Create an empty list to receive positive detection windows\n",
    "    on_windows = []\n",
    "    #2) Iterate over all windows in the list\n",
    "    for window in windows:\n",
    "        #3) Extract the test window from original image\n",
    "        test_img = cv2.resize(img[window[0][1]:window[1][1], window[0][0]:window[1][0]], (64, 64))      \n",
    "        #4) Extract features for that window using single_img_features()\n",
    "        features = single_img_features(test_img, color_space=color_space, \n",
    "                            spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                            orient=orient, pix_per_cell=pix_per_cell, \n",
    "                            cell_per_block=cell_per_block, \n",
    "                            hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                            hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "        X_pred.append(features)\n",
    "    #5) Predict car or notcar\n",
    "    predictions = clf.predict(scaler.transform(np.array(X_pred)))\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        if prediction == 1:\n",
    "            on_windows.append(windows[i])\n",
    "\n",
    "    #8) Return windows for positive detections\n",
    "    return on_windows\n",
    "\n",
    "images = []\n",
    "# images = ['notes/bbox-example-image.jpg']\n",
    "images += glob.glob('test_images/*.jpg')\n",
    "# images += ['test_images/test1.jpg']\n",
    "\n",
    "results = []\n",
    "for image in images:\n",
    "    image = mpimg.imread(image)\n",
    "    #print(image.shape[0:2][::-1])\n",
    "    # Uncomment the following line if you extracted training\n",
    "    # data from .png images (scaled 0 to 1 by mpimg) and the\n",
    "    # image you are searching is a .jpg (scaled 0 to 255)\n",
    "    image = image.astype(np.float32)/255\n",
    "    windows = []\n",
    "    t=time.time()\n",
    "    for w_size in list([256,128,96]):\n",
    "        found_windows  = slide_window(image, x_start_stop=[256, 1024], y_start_stop=[375, 631],\n",
    "                            xy_window=(w_size, w_size), xy_overlap=(0.77, 0.77))\n",
    "        print('Found {} windows for {} window...'.format(len(found_windows), str(w_size)))\n",
    "        windows+=found_windows\n",
    "    t2 = time.time()\n",
    "    print(round(t2-t, 2), 'Seconds to aggregate {} sliding windows...'.format(len(windows)))\n",
    "    \n",
    "    t=time.time()\n",
    "    hot_windows = search_windows(image, windows, svc, X_scaler, color_space=params.colorspace, \n",
    "                            spatial_size=params.spatial_size, hist_bins=params.hist_bins, \n",
    "                            orient=params.orient, pix_per_cell=params.pix_per_cell, \n",
    "                            cell_per_block=params.cell_per_block, \n",
    "                            hog_channel=params.hog_channel, spatial_feat=params.spatial_feat, \n",
    "                            hist_feat=params.hist_feat, hog_feat=params.hog_feat)                       \n",
    "    t2 = time.time()\n",
    "    print(round(t2-t, 2), 'Seconds to search and locate {} windows...'.format(len(hot_windows)))\n",
    "    results.append((image, hot_windows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for image, hot_windows in results:\n",
    "    draw_image = np.copy(image)\n",
    "    t=time.time()\n",
    "    window_img = draw_boxes(image, hot_windows, color=(0, 0, 255), thick=6)                    \n",
    "    t2 = time.time()\n",
    "    #     print(round(t2-t, 2), 'Seconds to draw boxes...')\n",
    "\n",
    "    PlotImageOp(window_img, title=\"Detected Vehicles\").perform()\n",
    "\n",
    "    heat = np.zeros_like(image[:,:,0]).astype(np.float)\n",
    "    heatmap = add_heat(heat, hot_windows)\n",
    "    heatmap = apply_threshold(heatmap, 3)\n",
    "    labels = label(heatmap)\n",
    "    print(labels[1], 'cars found')\n",
    "    # plt.imshow(labels[0], cmap='gray')\n",
    "    # plt.show()\n",
    "    #PlotImageOp(labels[0], cmap='gray').perform()\n",
    "    #print(labels)\n",
    "\n",
    "    final_map = np.clip(heatmap-2, 0, 255)\n",
    "    PlotImageOp(final_map, cmap='hot').perform()\n",
    "\n",
    "    draw_img = draw_labeled_bboxes(np.copy(image), labels)\n",
    "    # Display the image\n",
    "    PlotImageOp(draw_img, cmap=None).perform()\n",
    "    # plt.imshow(draw_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Show some examples of test images to demonstrate how your pipeline is working.  What did you do to optimize the performance of your classifier?\n",
    "\n",
    "Ultimately I searched on two scales using YCrCb 3-channel HOG features plus spatially binned color and histograms of color in the feature vector, which provided a nice result.  Here are some example images:\n",
    "\n",
    "![alt text][image4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "### Video Implementation\n",
    "\n",
    "#### 1. Provide a link to your final video output.  Your pipeline should perform reasonably well on the entire project video (somewhat wobbly or unstable bounding boxes are ok as long as you are identifying the vehicles most of the time with minimal false positives.)\n",
    "\n",
    "Here's a [link to my video result](https://youtu.be/jsrWyRCsjJo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format('project_video_final.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "class VehicleDetectionOp(PipelineOp):\n",
    "    def __init__(self, svc, X_scaler, params):\n",
    "        PipelineOp.__init__(self)\n",
    "        self.__svc = svc\n",
    "        self.__X_scaler = X_scaler\n",
    "        self.__params = params\n",
    "        self.__vis = vis\n",
    "        self.__img = None\n",
    "    \n",
    "    def detect_vehicles(self, image):\n",
    "        self.__img = image\n",
    "        return self.perform().output()\n",
    "    \n",
    "    def perform(self):\n",
    "        result = self.__img\n",
    "        if self.__img is not None:\n",
    "            svc = self.__svc\n",
    "            X_scaler = self.__X_scaler\n",
    "            params = self.__params\n",
    "            vis = self.__vis\n",
    "\n",
    "            # Uncomment the following line if you extracted training\n",
    "            # data from .png images (scaled 0 to 1 by mpimg) and the\n",
    "            # image you are searching is a .jpg (scaled 0 to 255)\n",
    "            image = image.astype(np.float32)/255\n",
    "            # Build our collection of search windows.\n",
    "            # Since I know I'm in the far left lane in the project video, \n",
    "            # I decided to scan only the center and right lanes for \n",
    "            # performance purposes.\n",
    "            #\n",
    "            # After trying out over 100 different window kernel size and \n",
    "            # overlap permutations, I discovered that having a very high \n",
    "            # overlap (i.e., >=0.7) yielded more possible detections and \n",
    "            # the greatest anomoly detection accuracy after applying \n",
    "            # the heat map.\n",
    "            #\n",
    "            # Having a higher overlap has its pros and cons so the below \n",
    "            # windows have a healthy balance of performance and accuracy. \n",
    "            #\n",
    "            # I also noticed that cars are not actually square so sliding \n",
    "            # a square kernel across non-square objects introduces errors. \n",
    "            # So, after trying out 7-8 different aspect ratios, I settled \n",
    "            # on a simple 5:4 aspect ratio giving me slighty more accurate \n",
    "            # readings across each frame.\n",
    "            #\n",
    "            # If I had to take aguess right now, I'd say my detection \n",
    "            # algorithm is as accurate as my LinearSVC classifier (~99.21%). \n",
    "            # There are still a few frames where the algorithm misses its \n",
    "            # target but I am satisfied with my first pass and look forward \n",
    "            # to peer feedback for tips and tricks.\n",
    "            #\n",
    "            # In addition, introducing a Vehicle class which is essentially \n",
    "            # responsible for tracking a single vehicle over time will ensure \n",
    "            # that even if I miss-calculate one our of seven frame, I'd \n",
    "            # essentially keep a running average from the previous 6 frames \n",
    "            # to fall back to.\n",
    "            overlap = 0.9\n",
    "            windows = []\n",
    "            # I start off with a single 64px tall window search space with 0.9 \n",
    "            # overlap to slide a 120x96 kernel across the horizon to pick up \n",
    "            # vehicles farther away. I wanted to apply the highest overlap at \n",
    "            # the smaller kernel size in the smallest search space because it \n",
    "            # is more accurate but also takes a lot longer to execute.\n",
    "            windows += slide_window(image, x_start_stop=[768, None], y_start_stop=[375, 439], xy_window=(int(96*1.25), 96), xy_overlap=(overlap, overlap)) \n",
    "            # Next, I define a 128px tall search space with essentially a 0.81 \n",
    "            # overlap to slide a 120x96 kernel accross. This essentially adds a \n",
    "            # bit more confidence around any detections from the previous window \n",
    "            # search. This also means that it was necessary for me to increase \n",
    "            # my heatmap threshold to remove any anomolies around the actual \n",
    "            # vehicle (e.g., road signs).\n",
    "            windows += slide_window(image, x_start_stop=[768, None], y_start_stop=[375, 567], xy_window=(int(96*1.25), 96), xy_overlap=(overlap*0.9, overlap*0.9))\n",
    "            # Finally, I sweep a 280x224 kernel across the entire search space \n",
    "            # with the full 0.9 overlap to pick up vehicles that may be right \n",
    "            # next to me or just entering the frame (i.e., closer to me).\n",
    "            windows += slide_window(image, x_start_stop=[768, None], y_start_stop=[375, 695], xy_window=(int(224*1.25), 224), xy_overlap=(overlap*1., overlap*1.))\n",
    "            # With the optimal windows identified, let's predict whether there's \n",
    "            # a vehicle inside of each window.\n",
    "            #\n",
    "            # This is by far the most process intensive method so it's imperative \n",
    "            # we address all performance concerns prior to reaching this phase.\n",
    "            # \n",
    "            # As it stands right now with the current windows, it take ~2-2.5s per \n",
    "            # frame.\n",
    "            t=time.time()\n",
    "            hot_windows = search_windows(image, windows, svc, X_scaler, color_space=params.colorspace, \n",
    "                                    spatial_size=params.spatial_size, hist_bins=params.hist_bins, \n",
    "                                    orient=params.orient, pix_per_cell=params.pix_per_cell, \n",
    "                                    cell_per_block=params.cell_per_block, \n",
    "                                    hog_channel=params.hog_channel, spatial_feat=params.spatial_feat, \n",
    "                                    hist_feat=params.hist_feat, hog_feat=params.hog_feat)                       \n",
    "            t2 = time.time()\n",
    "            print(round(t2-t, 2), 'Seconds to search and locate {} windows...'.format(len(hot_windows)))\n",
    "\n",
    "            # visualize the detected windows if visualization is enabled\n",
    "            if vis:\n",
    "                # Draw the detected windows on top of the original image.\n",
    "                window_img = draw_boxes(image, hot_windows, color=(0, 0, 255), thick=3)\n",
    "                PlotImageOp(window_img, title=\"Detected Vehicles\").perform()\n",
    "\n",
    "            # A clever algorithm I was about to implement prior to obtaining optimal detections \n",
    "            # with the current searchable windows was to actually track the heat map over \n",
    "            # time instead of averaging windows over time. I didnt' end up using it in the end \n",
    "            # but I am leaving my code in here in hopes I can experiment with it later on.\n",
    "            #\n",
    "            # The idea behind this algorithm is to start off with a base threshold and \n",
    "            # to continue to add +1 to new detections to the heat map for n frames then \n",
    "            # start from scratch after each nth frame (i.e., cool the heatmap down).\n",
    "            base_thresh = 5\n",
    "            if True: #self.heat == None or (self.current_frame%base_thresh) == 0:\n",
    "                self.heat = np.zeros_like(image[:,:,0]).astype(np.float)\n",
    "            heat = self.heat\n",
    "\n",
    "            # have a more lenient theshold by decaying over time\n",
    "            heat_thresh = (base_thresh+((base_thresh*(self.current_frame%base_thresh))*((self.current_frame%base_thresh)/base_thresh)))\n",
    "\n",
    "            # Weed out anomolies by excepting detections where at least \n",
    "            # 5 windows were predicted to have a vehicle in it.\n",
    "            heatmap = add_heat(heat, hot_windows)\n",
    "            heatmap = apply_threshold(heatmap, 5)\n",
    "            self.heat = heatmap\n",
    "            labels = label(heatmap)\n",
    "            print(labels[1], 'cars found')\n",
    "\n",
    "            # visualize the heatmap if visualization is enabled\n",
    "            if vis:\n",
    "                final_map = np.clip(heatmap-2, 0, 255)\n",
    "                PlotImageOp(final_map, cmap='hot').perform()\n",
    "\n",
    "            # Draw the labels onto the original image\n",
    "            result = draw_labeled_bboxes(np.copy(image*255), labels)\n",
    "        return self._apply_output(result)\n",
    "        \n",
    "\n",
    "class PipelineRunner:\n",
    "    def __init__(self, lane_assist_op, vehicle_detection_op, detect_lane=False, processed_images_save_dir=None):\n",
    "        self.lane_assist_op = lane_assist_op\n",
    "        self.vehicl_detection_op = vehicle_detection_op\n",
    "        \n",
    "        self.current_frame = -1\n",
    "#         self.vis = vis\n",
    "        self.detect_lane = detect_lane\n",
    "        self.__processed_images_save_dir = processed_images_save_dir\n",
    "        \n",
    "        self.svc = svc\n",
    "        self.X_scaler = X_scaler\n",
    "        self.params = params\n",
    "        self.heat = None\n",
    "        \n",
    "    def process_video(self, src_video_path, dst_video_path, audio=False):\n",
    "        self.current_frame = -1\n",
    "        self.__processed_images_save_dir = os.path.basename(src_video_path).split('.')[0]+'/'\n",
    "        VideoFileClip(src_video_path).fl_image(self.process_image).write_videofile(dst_video_path, audio=audio)\n",
    "    \n",
    "    def process_image(self, image):\n",
    "        self.current_frame += 1\n",
    "        self.__save_image(image, 'IN')\n",
    "        \n",
    "        # Detect vehicles\n",
    "        # image = self.__detect_vehicles(image)\n",
    "        image = self.vehicle_detection_op.detect_vehicles()\n",
    "        \n",
    "        # Detect lane\n",
    "        if self.detect_lane == True:\n",
    "            image = self.lane_assist_op.process_image(\n",
    "                image,\n",
    "                '{}{}'.format(self.__processed_images_save_dir, self.current_frame)\n",
    "            ).output()\n",
    "        \n",
    "        self.__save_image(image, 'OUT')\n",
    "        return image\n",
    "        \n",
    "    def __detect_vehicles(self, image):\n",
    "        svc = self.svc\n",
    "        X_scaler = self.X_scaler\n",
    "        params = self.params\n",
    "        \n",
    "        # Uncomment the following line if you extracted training\n",
    "        # data from .png images (scaled 0 to 1 by mpimg) and the\n",
    "        # image you are searching is a .jpg (scaled 0 to 255)\n",
    "        image = image.astype(np.float32)/255\n",
    "\n",
    "        # Build our collection of search windows.\n",
    "        # Since I know I'm in the far left lane in the project video, \n",
    "        # I decided to scan only the center and right lanes for \n",
    "        # performance purposes.\n",
    "        #\n",
    "        # After trying out over 100 different window kernel size and \n",
    "        # overlap permutations, I discovered that having a very high \n",
    "        # overlap (i.e., >=0.7) yielded more possible detections and \n",
    "        # the greatest anomoly detection accuracy after applying \n",
    "        # the heat map.\n",
    "        #\n",
    "        # Having a higher overlap has its pros and cons so the below \n",
    "        # windows have a healthy balance of performance and accuracy. \n",
    "        #\n",
    "        # I also noticed that cars are not actually square so sliding \n",
    "        # a square kernel across non-square objects introduces errors. \n",
    "        # So, after trying out 7-8 different aspect ratios, I settled \n",
    "        # on a simple 5:4 aspect ratio giving me slighty more accurate \n",
    "        # readings across each frame.\n",
    "        #\n",
    "        # If I had to take aguess right now, I'd say my detection \n",
    "        # algorithm is as accurate as my LinearSVC classifier (~99.21%). \n",
    "        # There are still a few frames where the algorithm misses its \n",
    "        # target but I am satisfied with my first pass and look forward \n",
    "        # to peer feedback for tips and tricks.\n",
    "        #\n",
    "        # In addition, introducing a Vehicle class which is essentially \n",
    "        # responsible for tracking a single vehicle over time will ensure \n",
    "        # that even if I miss-calculate one our of seven frame, I'd \n",
    "        # essentially keep a running average from the previous 6 frames \n",
    "        # to fall back to.\n",
    "        overlap = 0.9\n",
    "        windows = []\n",
    "        \n",
    "        # I start off with a single 64px tall window search space with 0.9 \n",
    "        # overlap to slide a 120x96 kernel across the horizon to pick up \n",
    "        # vehicles farther away. I wanted to apply the highest overlap at \n",
    "        # the smaller kernel size in the smallest search space because it \n",
    "        # is more accurate but also takes a lot longer to execute.\n",
    "        windows += slide_window(image, x_start_stop=[768, None], y_start_stop=[375, 439], xy_window=(int(96*1.25), 96), xy_overlap=(overlap, overlap)) \n",
    "        \n",
    "        # Next, I define a 128px tall search space with essentially a 0.81 \n",
    "        # overlap to slide a 120x96 kernel accross. This essentially adds a \n",
    "        # bit more confidence around any detections from the previous window \n",
    "        # search. This also means that it was necessary for me to increase \n",
    "        # my heatmap threshold to remove any anomolies around the actual \n",
    "        # vehicle (e.g., road signs).\n",
    "        windows += slide_window(image, x_start_stop=[768, None], y_start_stop=[375, 567], xy_window=(int(96*1.25), 96), xy_overlap=(overlap*0.9, overlap*0.9))\n",
    "        \n",
    "        # Finally, I sweep a 280x224 kernel across the entire search space \n",
    "        # with the full 0.9 overlap to pick up vehicles that may be right \n",
    "        # next to me or just entering the frame (i.e., closer to me).\n",
    "        windows += slide_window(image, x_start_stop=[768, None], y_start_stop=[375, 695], xy_window=(int(224*1.25), 224), xy_overlap=(overlap*1., overlap*1.))\n",
    "        \n",
    "        # With the optimal windows identified, let's predict whether there's \n",
    "        # a vehicle inside of each window.\n",
    "        #\n",
    "        # This is by far the most process intensive method so it's imperative \n",
    "        # we address all performance concerns prior to reaching this phase.\n",
    "        # \n",
    "        # As it stands right now with the current windows, it take ~2-2.5s per \n",
    "        # frame.\n",
    "        t=time.time()\n",
    "        hot_windows = search_windows(image, windows, svc, X_scaler, color_space=params.colorspace, \n",
    "                                spatial_size=params.spatial_size, hist_bins=params.hist_bins, \n",
    "                                orient=params.orient, pix_per_cell=params.pix_per_cell, \n",
    "                                cell_per_block=params.cell_per_block, \n",
    "                                hog_channel=params.hog_channel, spatial_feat=params.spatial_feat, \n",
    "                                hist_feat=params.hist_feat, hog_feat=params.hog_feat)                       \n",
    "        t2 = time.time()\n",
    "        print(round(t2-t, 2), 'Seconds to search and locate {} windows...'.format(len(hot_windows)))\n",
    "        \n",
    "        # visualize the detected windows if visualization is enabled\n",
    "        if self.vis:\n",
    "            # Draw the detected windows on top of the original image.\n",
    "            window_img = draw_boxes(image, hot_windows, color=(0, 0, 255), thick=3)\n",
    "            PlotImageOp(window_img, title=\"Detected Vehicles\").perform()\n",
    "        \n",
    "        # A clever algorithm I was about to implement prior to obtaining optimal detections \n",
    "        # with the current searchable windows was to actually track the heat map over \n",
    "        # time instead of averaging windows over time. I didnt' end up using it in the end \n",
    "        # but I am leaving my code in here in hopes I can experiment with it later on.\n",
    "        #\n",
    "        # The idea behind this algorithm is to start off with a base threshold and \n",
    "        # to continue to add +1 to new detections to the heat map for n frames then \n",
    "        # start from scratch after each nth frame (i.e., cool the heatmap down).\n",
    "        base_thresh = 5\n",
    "        if True: #self.heat == None or (self.current_frame%base_thresh) == 0:\n",
    "            self.heat = np.zeros_like(image[:,:,0]).astype(np.float)\n",
    "        heat = self.heat\n",
    "        \n",
    "        # have a more lenient theshold by decaying over time\n",
    "        heat_thresh = (base_thresh+((base_thresh*(self.current_frame%base_thresh))*((self.current_frame%base_thresh)/base_thresh)))\n",
    "        \n",
    "        # Weed out anomolies by excepting detections where at least \n",
    "        # 5 windows were predicted to have a vehicle in it.\n",
    "        heatmap = add_heat(heat, hot_windows)\n",
    "        heatmap = apply_threshold(heatmap, 5)\n",
    "        self.heat = heatmap\n",
    "        labels = label(heatmap)\n",
    "        print(labels[1], 'cars found')\n",
    "\n",
    "        # visualize the heatmap if visualization is enabled\n",
    "        if self.vis:\n",
    "            final_map = np.clip(heatmap-2, 0, 255)\n",
    "            PlotImageOp(final_map, cmap='hot').perform()\n",
    "\n",
    "        # Draw the labels onto the original image\n",
    "        detections = draw_labeled_bboxes(np.copy(image*255), labels)\n",
    "        \n",
    "        return detections\n",
    "    \n",
    "    def __save_image(self, image, name):\n",
    "        if self.__processed_images_save_dir != None:\n",
    "            cv2.imwrite('processed_images/{}{}_{}.jpg'.format(self.__processed_images_save_dir, self.current_frame, name), cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            \n",
    "# Lane detection op (from P4)\n",
    "lane_assist_op = LaneDetectionOp(\n",
    "    calibration_op,\n",
    "    margin=100,\n",
    "    kernel_size=15,\n",
    "    sobelx_thresh=(20,100),\n",
    "    sobely_thresh=(20,100),\n",
    "    mag_grad_thresh=(20,250),\n",
    "    dir_grad_thresh=(0.3, 1.3),\n",
    "    color_space='HSV',\n",
    "    color_channel=2\n",
    ")\n",
    "\n",
    "vehicle_detection_op = VehicleDetectionOp(\n",
    "    svc, \n",
    "    X_scaler, \n",
    "    params, \n",
    "    detect_lane=False,\n",
    "    vis=True\n",
    ")\n",
    "\n",
    "# See how well my pipeline performs against all .jpg images inside test_images directory\n",
    "if False:\n",
    "    images = []\n",
    "    for i in range(1253,1261):\n",
    "        images.append('processed_images/project_video/{}_IN.jpg'.format(i))\n",
    "    \n",
    "#     for i in range(290,1261):\n",
    "#         if i%25==0:\n",
    "#             images.append('processed_images/project_video/{}_IN.jpg'.format(i))\n",
    "#     images += ['processed_images/project_video/700_IN.jpg']\n",
    "#     images += ['processed_images/project_video/1200_IN.jpg']\n",
    "#     images += ['processed_images/project_video/1225_IN.jpg']\n",
    "    # images += ['processed_images/test_video/0_IN.jpg']\n",
    "    # images += ['notes/bbox-example-image.jpg']\n",
    "    # images += glob.glob('test_images/*.jpg')\n",
    "    # images += glob.glob('test_images/test3.jpg')\n",
    "    \n",
    "    pipeline = PipelineRunner(lane_assist_op, vehicle_detection_op, detect_lane=False, processed_images_save_dir='samples')\n",
    "    \n",
    "    for img_path in images:\n",
    "        result = pipeline.process_image(mpimg.imread(img_path))\n",
    "        PlotImageOp(result*255, title=\"{} - FINAL\".format(img_path), cmap=None).perform()\n",
    "\n",
    "        # Run pipeline against the main project_video.mp4\n",
    "if False:\n",
    "    pipeline.process_video('test_video.mp4', 'test_video_final.mp4')\n",
    "    \n",
    "# Run pipeline against the main project_video.mp4\n",
    "if False:\n",
    "    pipeline.process_video('project_video.mp4', 'project_video_final.mp4')\n",
    "\n",
    "# Run pipeline against the challenge_video.mp4\n",
    "if False:\n",
    "    pipeline.process_video('challenge_video.mp4', 'challenge_video_final.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Describe how (and identify where in your code) you implemented some kind of filter for false positives and some method for combining overlapping bounding boxes.\n",
    "\n",
    "I recorded the positions of positive detections in each frame of the video.  From the positive detections I created a heatmap and then thresholded that map to identify vehicle positions.  I then used `scipy.ndimage.measurements.label()` to identify individual blobs in the heatmap.  I then assumed each blob corresponded to a vehicle.  I constructed bounding boxes to cover the area of each blob detected.  \n",
    "\n",
    "Here's an example result showing the heatmap from a series of frames of video, the result of `scipy.ndimage.measurements.label()` and the bounding boxes then overlaid on the last frame of video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are six frames and their corresponding heatmaps:\n",
    "\n",
    "![alt text][image5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is the output of `scipy.ndimage.measurements.label()` on the integrated heatmap from all six frames:\n",
    "![alt text][image6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here the resulting bounding boxes are drawn onto the last frame in the series:\n",
    "![alt text][image7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Discussion\n",
    "\n",
    "#### 1. Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?\n",
    "\n",
    "Here I'll talk about the approach I took, what techniques I used, what worked and why, where the pipeline might fail and how I might improve it if I were going to pursue this project further. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:carnd]",
   "language": "python",
   "name": "conda-env-carnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
